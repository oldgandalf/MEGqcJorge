{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is trying out different pipeline approaches\n",
    "\n",
    "# Cropped data is used here (5 minutes only), tried on whole data - takes forever.\n",
    "\n",
    "\n",
    "#Load data, make folders\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import mne\n",
    "import pyprep as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Functions.main_meg_qc import initial_stuff\n",
    "\n",
    "\n",
    "#Not working:\n",
    "data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003483/sub-009/ses-1/meg/sub-009_ses-1_task-deduction_run-1_meg.fif'\n",
    "#(calculates for 5h to no results - 2nd tutorial)\n",
    "\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003352/sub-1/ses-01/meg/sub-1_ses-01_task-ColorSpirals_run-00_meg.fif'\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003352/sub-4/ses-02/meg/sub-4_ses-02_task-ColorSpirals_run-01_meg.fif'\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003392/sub-01/meg/sub-01_task-localizer_meg.fif'\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003645/sub-002/meg/sub-002_task-FacePerception_run-1_meg.fif'\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003694/sub-02/meg/sub-02_task-MEM_run-01_meg.fif'\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004107/sub-mind002/ses-01/meg/sub-mind002_ses-01_task-auditory_meg.fif'\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004276/sub-002/meg/sub-002_task-words_meg.fif'\n",
    "\n",
    "#WORKING 1st tutorial only:\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003703/sub-a68d5xp5/meg/sub-a68d5xp5_task-listeningToSpeech_run-01_meg.fif'\n",
    "\n",
    "#WORKING 1st tutorial only:\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003682/sub-001/ses-01/meg/sub-001_ses-01_task-AversiveLearningReplay_run-01_meg.fif'\n",
    "\n",
    "\n",
    "raw = mne.io.read_raw_fif(data_file, on_split_missing='ignore')\n",
    "\n",
    "#crop the data to calculate faster\n",
    "raw_cropped = raw.copy()\n",
    "#raw_cropped.crop(tmin=1100, tmax=None) \n",
    "\n",
    "raw_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate movement - like in tutorial 2 from:\n",
    "# https://mne.tools/stable/generated/mne.preprocessing.annotate_movement.html\n",
    "\n",
    "from mne.preprocessing import annotate_movement, compute_average_dev_head_t\n",
    "\n",
    "# Get cHPI time series and compute average\n",
    "chpi_locs = mne.chpi.extract_chpi_locs_ctf(raw)\n",
    "head_pos = mne.chpi.compute_head_pos(raw.info, chpi_locs)\n",
    "original_head_dev_t = mne.transforms.invert_transform(\n",
    "    raw.info['dev_head_t'])\n",
    "average_head_dev_t = mne.transforms.invert_transform(\n",
    "    compute_average_dev_head_t(raw, head_pos))\n",
    "fig = mne.viz.plot_head_positions(head_pos)\n",
    "for ax, val, val_ori in zip(fig.axes[::2], average_head_dev_t['trans'][:3, 3],\n",
    "                            original_head_dev_t['trans'][:3, 3]):\n",
    "    ax.axhline(1000 * val, color='r')\n",
    "    ax.axhline(1000 * val_ori, color='g')\n",
    "\n",
    "# The green horizontal lines represent the original head position, whereas the\n",
    "# red lines are the new head position averaged over all the time points.\n",
    "\n",
    "mean_distance_limit = 0.0015  # in meters\n",
    "annotation_movement, hpi_disp = annotate_movement(\n",
    "    raw, head_pos, mean_distance_limit=mean_distance_limit)\n",
    "raw.set_annotations(annotation_movement)\n",
    "raw.plot(n_channels=100, duration=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try another tutorial: \n",
    "# (takes some minutes depends how big is raw_cropped: 2-40 min):\n",
    "# https://mne.tools/stable/auto_tutorials/preprocessing/59_head_positions.html#sphx-glr-auto-tutorials-preprocessing-59-head-positions-py\n",
    "\n",
    "# cHPI - continuous head position indicator (HPI) coil channels, data in teslas\n",
    "\n",
    "# 'We can use mne.chpi.get_chpi_info to retrieve the coil frequencies, the index of \n",
    "# the channel indicating when which coil was switched on, and the respective “event codes” \n",
    "# associated with each coil’s activity.'\n",
    "chpi_freqs, ch_idx, chpi_codes = mne.chpi.get_chpi_info(info=raw_cropped.info, on_missing='warn', verbose=None)\n",
    "# Output:\n",
    "# - The frequency used for each individual cHPI coil.\n",
    "# - The index of the STIM channel containing information about when which cHPI coils were switched on.\n",
    "# - The values coding for the “on” state of each individual cHPI coil.\n",
    "\n",
    "# The values coding for the “on” state of each individual cHPI coil.\n",
    "print(f'cHPI coil frequencies extracted from raw: {chpi_freqs} Hz')\n",
    "\n",
    "#We only got 5, not 9 HPI (see error in cell above)\n",
    "\n",
    "\n",
    "#extract the HPI coil amplitudes as a function of time:\n",
    "print('Extract the HPI coil amplitudes as a function of time:')\n",
    "chpi_amplitudes=mne.chpi.compute_chpi_amplitudes(raw_cropped)\n",
    "#chpi_amplitudes=mne.chpi.compute_chpi_amplitudes(raw_cropped, t_step_min=0.01, t_window='auto', ext_order=1, tmin=0, tmax=None, verbose=None)\n",
    "\n",
    "\n",
    "#compute time-varying HPI coil locations from these\n",
    "print('Compute time-varying HPI coil locations from these')\n",
    "#chpi_locs=mne.chpi.compute_chpi_locs(raw_cropped.info, chpi_amplitudes, t_step_max=1.0, too_close='raise', adjust_dig=False, verbose=None)\n",
    "chpi_locs=mne.chpi.compute_chpi_locs(raw_cropped.info, chpi_amplitudes)\n",
    "\n",
    "\n",
    "print('Compute head positions from the coil locations:')\n",
    "#compute head positions from the coil locations:\n",
    "head_pos = mne.chpi.compute_head_pos(raw_cropped.info, chpi_locs, verbose=True)\n",
    "print('head_positions computed:', head_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing continuous head position: doesnt work\n",
    "\n",
    "mne.viz.plot_head_positions(head_pos, mode='traces')\n",
    "#mne.viz.plot_head_positions(head_pos, mode='field')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now again try to annotate movement: also error - It didnt calculate any positions!\n",
    "\n",
    "mean_distance_limit = 0.0015  # in meters\n",
    "annotation_movement, hpi_disp = annotate_movement(\n",
    "    raw_cropped, head_pos, mean_distance_limit=mean_distance_limit)\n",
    "raw_cropped.set_annotations(raw_cropped.annotations + annotation_movement)\n",
    "raw_cropped.plot(n_channels=100, duration=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mne_new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 | packaged by conda-forge | (main, May 27 2022, 17:00:52) \n[Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d401ab1bf6dd7bb97662b0feb1321a641d60d04842bfa92b47f7871360972b5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
