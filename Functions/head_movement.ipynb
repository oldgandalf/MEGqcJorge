{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is trying out different pipeline approaches\n",
    "\n",
    "# Cropped data is used here (5 minutes only), tried on whole data - takes forever.\n",
    "\n",
    "\n",
    "#Load data, make folders\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import mne\n",
    "import pyprep as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file /Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003483/sub-009/ses-1/meg/sub-009_ses-1_task-deduction_run-1_meg.fif...\n",
      "    Range : 60000 ... 1255999 =     60.000 ...  1255.999 secs\n",
      "Ready.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"table table-hover table-striped table-sm table-responsive small\">\n",
       "    <tr>\n",
       "        <th>Measurement date</th>\n",
       "        \n",
       "        <td>February 21, 2019  11:11:50 GMT</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Experimenter</th>\n",
       "        \n",
       "        <td>Common account for MEG work (meg)</td>\n",
       "        \n",
       "    </tr>\n",
       "        <th>Participant</th>\n",
       "        \n",
       "            \n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Digitized points</th>\n",
       "        \n",
       "        <td>0 points</td>\n",
       "        \n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Good channels</th>\n",
       "        <td>204 Gradiometers, 102 Magnetometers, 1 EOG, 1 ECG, 3 Stimulus, 9 CHPI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>Bad channels</th>\n",
       "        <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>EOG channels</th>\n",
       "        <td>EOG061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <th>ECG channels</th>\n",
       "        <td>ECG062</td>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Sampling frequency</th>\n",
       "        <td>1000.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Highpass</th>\n",
       "        <td>0.10 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Lowpass</th>\n",
       "        <td>330.00 Hz</td>\n",
       "    </tr>\n",
       "    \n",
       "    \n",
       "    \n",
       "    <tr>\n",
       "        <th>Filenames</th>\n",
       "        <td>sub-009_ses-1_task-deduction_run-1_meg.fif</td>\n",
       "    </tr>\n",
       "    \n",
       "    <tr>\n",
       "        <th>Duration</th>\n",
       "        <td>00:19:55 (HH:MM:SS)</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Raw | sub-009_ses-1_task-deduction_run-1_meg.fif, 320 x 1196000 (1196.0 s), ~5.3 MB, data not loaded>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from Functions.main_meg_qc import initial_stuff\n",
    "from data_load_and_folders import load_meg_data\n",
    "duration=5 #in minutes\n",
    "#n_events, df_epochs_mags, df_epochs_grads, epochs_mags, epochs_grads, mags, grads, filtered_d, filtered_d_resamp, raw_cropped, raw=initial_stuff(duration)\n",
    "\n",
    "#Not working:\n",
    "data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003483/sub-009/ses-1/meg/sub-009_ses-1_task-deduction_run-1_meg.fif'\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003352/sub-1/ses-01/meg/sub-1_ses-01_task-ColorSpirals_run-00_meg.fif'\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003352/sub-4/ses-02/meg/sub-4_ses-02_task-ColorSpirals_run-01_meg.fif'\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003392/sub-01/meg/sub-01_task-localizer_meg.fif'\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003645/sub-002/meg/sub-002_task-FacePerception_run-1_meg.fif'\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003694/sub-02/meg/sub-02_task-MEM_run-01_meg.fif'\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004107/sub-mind002/ses-01/meg/sub-mind002_ses-01_task-auditory_meg.fif'\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds004276/sub-002/meg/sub-002_task-words_meg.fif'\n",
    "\n",
    "#WORKING 1st tutorial only:\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003703/sub-a68d5xp5/meg/sub-a68d5xp5_task-listeningToSpeech_run-01_meg.fif'\n",
    "\n",
    "#WORKING 2nd tutorial only:\n",
    "#data_file = '/Volumes/M2_DATA/MEG_QC_stuff/data/from openneuro/ds003682/sub-001/ses-01/meg/sub-001_ses-01_task-AversiveLearningReplay_run-01_meg.fif'\n",
    "\n",
    "\n",
    "raw, channels = load_meg_data(data_file)\n",
    "\n",
    "#crop the data to calculate faster\n",
    "raw_cropped = raw.copy()\n",
    "#raw_cropped.crop(tmin=1100, tmax=None) \n",
    "\n",
    "raw_cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not find all 9 cHPI channels",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/jenya/Local Storage/Job Uni Rieger lab/MEG QC code/Functions/head_movement.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jenya/Local%20Storage/Job%20Uni%20Rieger%20lab/MEG%20QC%20code/Functions/head_movement.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmne\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m annotate_movement, compute_average_dev_head_t\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jenya/Local%20Storage/Job%20Uni%20Rieger%20lab/MEG%20QC%20code/Functions/head_movement.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Get cHPI time series and compute average\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jenya/Local%20Storage/Job%20Uni%20Rieger%20lab/MEG%20QC%20code/Functions/head_movement.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m chpi_locs \u001b[39m=\u001b[39m mne\u001b[39m.\u001b[39;49mchpi\u001b[39m.\u001b[39;49mextract_chpi_locs_ctf(raw)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jenya/Local%20Storage/Job%20Uni%20Rieger%20lab/MEG%20QC%20code/Functions/head_movement.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m head_pos \u001b[39m=\u001b[39m mne\u001b[39m.\u001b[39mchpi\u001b[39m.\u001b[39mcompute_head_pos(raw\u001b[39m.\u001b[39minfo, chpi_locs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jenya/Local%20Storage/Job%20Uni%20Rieger%20lab/MEG%20QC%20code/Functions/head_movement.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m original_head_dev_t \u001b[39m=\u001b[39m mne\u001b[39m.\u001b[39mtransforms\u001b[39m.\u001b[39minvert_transform(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jenya/Local%20Storage/Job%20Uni%20Rieger%20lab/MEG%20QC%20code/Functions/head_movement.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     raw\u001b[39m.\u001b[39minfo[\u001b[39m'\u001b[39m\u001b[39mdev_head_t\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m<decorator-gen-511>:12\u001b[0m, in \u001b[0;36mextract_chpi_locs_ctf\u001b[0;34m(raw, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mne_new/lib/python3.9/site-packages/mne/chpi.py:185\u001b[0m, in \u001b[0;36mextract_chpi_locs_ctf\u001b[0;34m(raw, verbose)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39m# make sure we get 9 channels\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(hpi_picks) \u001b[39m!=\u001b[39m \u001b[39m9\u001b[39m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mCould not find all 9 cHPI channels\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    187\u001b[0m \u001b[39m# get indices in alphabetical order\u001b[39;00m\n\u001b[1;32m    188\u001b[0m sorted_picks \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39msorted\u001b[39m(hpi_picks,\n\u001b[1;32m    189\u001b[0m                                key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m k: raw\u001b[39m.\u001b[39minfo[\u001b[39m'\u001b[39m\u001b[39mch_names\u001b[39m\u001b[39m'\u001b[39m][k]))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not find all 9 cHPI channels"
     ]
    }
   ],
   "source": [
    "# Annotate movement - like in tutorial 2 from:\n",
    "# https://mne.tools/stable/generated/mne.preprocessing.annotate_movement.html\n",
    "\n",
    "from mne.preprocessing import annotate_movement, compute_average_dev_head_t\n",
    "\n",
    "# Get cHPI time series and compute average\n",
    "chpi_locs = mne.chpi.extract_chpi_locs_ctf(raw)\n",
    "head_pos = mne.chpi.compute_head_pos(raw.info, chpi_locs)\n",
    "original_head_dev_t = mne.transforms.invert_transform(\n",
    "    raw.info['dev_head_t'])\n",
    "average_head_dev_t = mne.transforms.invert_transform(\n",
    "    compute_average_dev_head_t(raw, head_pos))\n",
    "fig = mne.viz.plot_head_positions(head_pos)\n",
    "for ax, val, val_ori in zip(fig.axes[::2], average_head_dev_t['trans'][:3, 3],\n",
    "                            original_head_dev_t['trans'][:3, 3]):\n",
    "    ax.axhline(1000 * val, color='r')\n",
    "    ax.axhline(1000 * val_ori, color='g')\n",
    "\n",
    "# The green horizontal lines represent the original head position, whereas the\n",
    "# red lines are the new head position averaged over all the time points.\n",
    "\n",
    "mean_distance_limit = 0.0015  # in meters\n",
    "annotation_movement, hpi_disp = annotate_movement(\n",
    "    raw, head_pos, mean_distance_limit=mean_distance_limit)\n",
    "raw.set_annotations(annotation_movement)\n",
    "raw.plot(n_channels=100, duration=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 HPI coils: 169 176 183 190 Hz\n",
      "cHPI coil frequencies extracted from raw: [169. 176. 183. 190.] Hz\n",
      "Extract the HPI coil amplitudes as a function of time:\n",
      "Using 4 HPI coils: 169 176 183 190 Hz\n",
      "Line interference frequencies: 50 100 150 200 250 300 Hz\n",
      "Using time window: 142.9 ms\n",
      "Fitting 4 HPI coil locations at up to 119593 time points (1196.0 sec duration)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c92ec3217154a85b10cc37f50db673e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | cHPI amplitudes : 0/119593 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute time-varying HPI coil locations from these\n",
      "Computing 4385 HPI location guesses (1 cm grid in a 10.7 cm sphere)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h2/khhmb4p510vg63hbv0qkftt80000gs/T/ipykernel_58668/370082608.py:31: RuntimeWarning: Something went wrong in the data-driven estimation of the data rank as it exceeds the theoretical rank from the info (303 > 70). Consider setting rank to \"auto\" or setting it explicitly as an integer.\n",
      "  chpi_locs=mne.chpi.compute_chpi_locs(raw_cropped.info, chpi_amplitudes)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HPIFIT: 4 coils digitized in order 1 2 3 4\n",
      "HPI consistency of isotrak and hpifit is OK.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edc73bad75104fd0a297d39ffe9479f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | cHPI locations  : 0/119593 [00:00<?,       ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try another tutorial: \n",
    "# (takes some minutes depends how big is raw_cropped: 2-40 min):\n",
    "# https://mne.tools/stable/auto_tutorials/preprocessing/59_head_positions.html#sphx-glr-auto-tutorials-preprocessing-59-head-positions-py\n",
    "\n",
    "# cHPI - continuous head position indicator (HPI) coil channels, data in teslas\n",
    "\n",
    "# 'We can use mne.chpi.get_chpi_info to retrieve the coil frequencies, the index of \n",
    "# the channel indicating when which coil was switched on, and the respective “event codes” \n",
    "# associated with each coil’s activity.'\n",
    "chpi_freqs, ch_idx, chpi_codes = mne.chpi.get_chpi_info(info=raw_cropped.info, on_missing='warn', verbose=None)\n",
    "# Output:\n",
    "# - The frequency used for each individual cHPI coil.\n",
    "# - The index of the STIM channel containing information about when which cHPI coils were switched on.\n",
    "# - The values coding for the “on” state of each individual cHPI coil.\n",
    "\n",
    "# The values coding for the “on” state of each individual cHPI coil.\n",
    "print(f'cHPI coil frequencies extracted from raw: {chpi_freqs} Hz')\n",
    "\n",
    "#We only got 5, not 9 HPI (see error in cell above)\n",
    "\n",
    "\n",
    "#extract the HPI coil amplitudes as a function of time:\n",
    "print('Extract the HPI coil amplitudes as a function of time:')\n",
    "chpi_amplitudes=mne.chpi.compute_chpi_amplitudes(raw_cropped)\n",
    "#chpi_amplitudes=mne.chpi.compute_chpi_amplitudes(raw_cropped, t_step_min=0.01, t_window='auto', ext_order=1, tmin=0, tmax=None, verbose=None)\n",
    "\n",
    "\n",
    "#compute time-varying HPI coil locations from these\n",
    "print('Compute time-varying HPI coil locations from these')\n",
    "#chpi_locs=mne.chpi.compute_chpi_locs(raw_cropped.info, chpi_amplitudes, t_step_max=1.0, too_close='raise', adjust_dig=False, verbose=None)\n",
    "chpi_locs=mne.chpi.compute_chpi_locs(raw_cropped.info, chpi_amplitudes)\n",
    "\n",
    "\n",
    "print('Compute head positions from the coil locations:')\n",
    "#compute head positions from the coil locations:\n",
    "head_pos = mne.chpi.compute_head_pos(raw_cropped.info, chpi_locs, verbose=True)\n",
    "print('head_positions computed:', head_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing continuous head position: doesnt work\n",
    "\n",
    "mne.viz.plot_head_positions(head_pos, mode='traces')\n",
    "#mne.viz.plot_head_positions(head_pos, mode='field')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now again try to annotate movement: also error - It didnt calculate any positions!\n",
    "\n",
    "mean_distance_limit = 0.0015  # in meters\n",
    "annotation_movement, hpi_disp = annotate_movement(\n",
    "    raw_cropped, head_pos, mean_distance_limit=mean_distance_limit)\n",
    "raw_cropped.set_annotations(raw_cropped.annotations + annotation_movement)\n",
    "raw_cropped.plot(n_channels=100, duration=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mne_new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d401ab1bf6dd7bb97662b0feb1321a641d60d04842bfa92b47f7871360972b5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
