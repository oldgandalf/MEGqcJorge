{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import from meg_qc, relative to the path of this file\n",
    "\n",
    "# # add meg_qc module to the path\n",
    "\n",
    "# from meg_qc.meg_qc_pipeline import make_derivative_meg_qc\n",
    "\n",
    "# config_file_path = 'meg_qc/settings.ini' \n",
    "# internal_config_file_path='meg_qc/settings_internal.ini' # internal settings in in\n",
    "# #raw, raw_cropped_filtered_resampled, QC_derivs, QC_simple, df_head_pos, head_pos, scores_muscle_all1, scores_muscle_all2, scores_muscle_all3, raw1, raw2, raw3, avg_ecg, avg_eog = make_derivative_meg_qc(config_file_path, internal_config_file_path)\n",
    "\n",
    "# for_report, Plotting_paths = make_derivative_meg_qc(config_file_path, internal_config_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def figure_x_axis(df, metric):\n",
    "     \n",
    "    if metric.lower() == 'psd':\n",
    "        # Figure out frequencies:\n",
    "        freq_cols = [column for column in df if column.startswith('PSD_Hz_')]\n",
    "        freqs = np.array([float(x.replace('PSD_Hz_', '')) for x in freq_cols])\n",
    "        return freqs\n",
    "    \n",
    "    elif metric.lower() == 'eog' or metric.lower() == 'ecg' or metric.lower() == 'muscle' or metric.lower() == 'head':\n",
    "        if metric.lower() == 'ecg':\n",
    "            prefix = 'mean ECG_sec_'\n",
    "        elif metric.lower() == 'eog': \n",
    "            prefix = 'mean EOG_sec_'\n",
    "        elif metric.lower() == 'muscle':\n",
    "            prefix = 'Muscle_sec_'\n",
    "        elif metric.lower() == 'head':\n",
    "            prefix = 'Head_sec_'\n",
    "        \n",
    "        time_cols = [column for column in df if column.startswith(prefix)]\n",
    "        time_vec = np.array([float(x.replace(prefix, '')) for x in time_cols])\n",
    "\n",
    "        return time_vec\n",
    "    \n",
    "    else:\n",
    "        print('Oh well IDK!')\n",
    "        return None\n",
    "\n",
    "\n",
    "#df = pd.read_csv('/Volumes/M2_DATA/ECGs_by_lobe.csv')\n",
    "df = pd.read_csv('/Volumes/M2_DATA/PSDs_by_lobe.csv')\n",
    "xaxis = figure_x_axis(df, 'psd')\n",
    "print(xaxis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_correlated_artifacts_into_3_groups_csv(df, metric):\n",
    "\n",
    "    \"\"\"\n",
    "    Collect artif_per_ch into 3 lists - for plotting:\n",
    "    - a third of all channels that are the most correlated with mean_rwave\n",
    "    - a third of all channels that are the least correlated with mean_rwave\n",
    "    - a third of all channels that are in the middle of the correlation with mean_rwave\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    artif_per_ch : list\n",
    "        List of objects of class Avg_artif\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    artif_per_ch : list\n",
    "        List of objects of class Avg_artif, ranked by correlation coefficient\n",
    "    most_correlated : list\n",
    "        List of objects of class Avg_artif that are the most correlated with mean_rwave\n",
    "    least_correlated : list\n",
    "        List of objects of class Avg_artif that are the least correlated with mean_rwave\n",
    "    middle_correlated : list\n",
    "        List of objects of class Avg_artif that are in the middle of the correlation with mean_rwave\n",
    "    corr_val_of_last_least_correlated : float\n",
    "        Correlation value of the last channel in the list of the least correlated channels\n",
    "    corr_val_of_last_middle_correlated : float\n",
    "        Correlation value of the last channel in the list of the middle correlated channels\n",
    "    corr_val_of_last_most_correlated : float\n",
    "        Correlation value of the last channel in the list of the most correlated channels\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #sort by correlation coef. Take abs of the corr coeff, because the channels might be just flipped due to their location against magnetic field::\n",
    "    #artif_per_ch.sort(key=lambda x: abs(x.corr_coef), reverse=True)\n",
    "\n",
    "    if metric.lower() != 'ecg' and metric.lower() != 'eog':\n",
    "        print('Wrong metric in split_correlated_artifacts_into_3_groups_csv()')\n",
    "\n",
    "\n",
    "    df_sorted = df.copy()    \n",
    "    df_sorted.sort_values(by = metric.lower()+'_corr_coeff') \n",
    "\n",
    "    most_correlated = df_sorted.copy()[:int(len(df_sorted)/3)]\n",
    "    least_correlated = df_sorted.copy()[-int(len(df_sorted)/3):]\n",
    "    middle_correlated = df_sorted.copy()[int(len(df_sorted)/3):-int(len(df_sorted)/3)]\n",
    "\n",
    "    #get correlation values of all most correlated channels:\n",
    "    all_most_correlated = most_correlated[metric.lower()+'_corr_coeff'].abs().tolist()\n",
    "    all_middle_correlated = middle_correlated[metric.lower()+'_corr_coeff'].abs().tolist()\n",
    "    all_least_correlated = least_correlated[metric.lower()+'_corr_coeff'].abs().tolist()\n",
    "\n",
    "    #find the correlation value of the last channel in the list of the most correlated channels:\n",
    "    # this is needed for plotting correlation values, to know where to put separation rectangles.\n",
    "    corr_val_of_last_most_correlated = max(all_most_correlated)\n",
    "    corr_val_of_last_middle_correlated = max(all_middle_correlated)\n",
    "    corr_val_of_last_least_correlated = max(all_least_correlated)\n",
    "\n",
    "    return most_correlated, middle_correlated, least_correlated, corr_val_of_last_most_correlated, corr_val_of_last_middle_correlated, corr_val_of_last_least_correlated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from meg_qc.source.universal_plots import plot_df_of_channels_data_as_lines_by_lobe_csv, get_tit_and_unit\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_affected_channels_csv(df, artifact_lvl: float, t: np.ndarray, m_or_g: str, fig_tit: str, flip_data: bool or str = 'flip', smoothed: bool = False, verbose_plots: bool = True):\n",
    "\n",
    "    \"\"\"\n",
    "    Plot the mean artifact amplitude for all affected (not affected) channels in 1 plot together with the artifact_lvl.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    artif_affected_channels : list\n",
    "        List of ECG/EOG artifact affected channels.\n",
    "    artifact_lvl : float\n",
    "        The threshold for the artifact amplitude: average over all channels*norm_lvl.\n",
    "    t : np.ndarray\n",
    "        Time vector.\n",
    "    m_or_g : str\n",
    "        Either 'mag' or 'grad'.\n",
    "    fig_tit: str\n",
    "        The title of the figure.\n",
    "    chs_by_lobe : dict\n",
    "        dictionary with channel objects sorted by lobe\n",
    "    flip_data : bool\n",
    "        If True, the absolute value of the data will be used for the calculation of the mean artifact amplitude. Default to 'flip'. \n",
    "        'flip' means that the data will be flipped if the peak of the artifact is negative. \n",
    "        This is donr to get the same sign of the artifact for all channels, then to get the mean artifact amplitude over all channels and the threshold for the artifact amplitude onbase of this mean\n",
    "        And also for the reasons of visualization: the artifact amplitude is always positive.\n",
    "    smoothed: bool\n",
    "        Plot smoothed data (true) or nonrmal (false)\n",
    "    verbose_plots : bool\n",
    "        True for showing plot in notebook.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fig : plotly.graph_objects.Figure\n",
    "        The plotly figure with the mean artifact amplitude for all affected (not affected) channels in 1 plot together with the artifact_lvl.\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    #if df and not df.empty: #if affected channels present:\n",
    "    if df is not None:\n",
    "\n",
    "        # #plot channels separated by lobes:\n",
    "        # affected_names_list = []\n",
    "        # affected_data_list = []\n",
    "        # for ch in artif_affected_channels:\n",
    "        #     affected_names_list.append(ch.name)\n",
    "        #     if smoothed is True:\n",
    "        #         affected_data_list.append(ch.artif_data_smoothed)\n",
    "        #     else:\n",
    "        #         affected_data_list.append(ch.artif_data)\n",
    "\n",
    "        # affected_data_arr = np.array(affected_data_list)\n",
    "\n",
    "        # df_affected=pd.DataFrame(affected_data_arr.T, columns=affected_names_list)\n",
    "\n",
    "        #fig = plot_df_of_channels_data_as_lines_by_lobe(chs_by_lobe, df_affected, t)\n",
    "        fig = plot_df_of_channels_data_as_lines_by_lobe_csv(None, 'ecg', t, m_or_g, df)\n",
    "\n",
    "        #TODO: add smoothed version here!!\n",
    "\n",
    "        #decorate the plot:\n",
    "        ch_type_tit, unit = get_tit_and_unit(m_or_g)\n",
    "        fig.update_layout(\n",
    "            xaxis_title='Time in seconds',\n",
    "            yaxis = dict(\n",
    "                showexponent = 'all',\n",
    "                exponentformat = 'e'),\n",
    "            yaxis_title='Mean artifact magnitude in '+unit,\n",
    "            title={\n",
    "                'text': fig_tit+str(len(df))+' '+ch_type_tit,\n",
    "                'y':0.85,\n",
    "                'x':0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top'})\n",
    "\n",
    "\n",
    "    else:\n",
    "        fig=go.Figure()\n",
    "        ch_type_tit, _ = get_tit_and_unit(m_or_g)\n",
    "        title=fig_tit+'0 ' +ch_type_tit\n",
    "        fig.update_layout(\n",
    "            title={\n",
    "            'text': title,\n",
    "            'x': 0.5,\n",
    "            'y': 0.9,\n",
    "            'xanchor': 'center',\n",
    "            'yanchor': 'top'})\n",
    "        \n",
    "    #in any case - add the threshold on the plot\n",
    "    fig.add_trace(go.Scatter(x=t, y=[(artifact_lvl)]*len(t), line=dict(color='red'), name='Thres=mean_peak/norm_lvl')) #add threshold level\n",
    "\n",
    "    if flip_data is False and artifact_lvl is not None: \n",
    "        fig.add_trace(go.Scatter(x=t, y=[(-artifact_lvl)]*len(t), line=dict(color='black'), name='-Thres=mean_peak/norm_lvl'))\n",
    "\n",
    "    if verbose_plots is True:\n",
    "        fig.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_artif_per_ch_correlated_lobes_csv(f_path: str, m_or_g: str, ecg_or_eog: str, flip_data: bool, verbose_plots: bool):\n",
    "\n",
    "    \"\"\"\n",
    "    THE FINAL\n",
    "    TODO!!!\n",
    "    smoothing\n",
    "    adjust colimn names like 'mean ECG_sec_' with/no undescores.\n",
    "    adjust str.lower() to .upper\n",
    "    make plot_affected_channels_csv() into just a function of adjusting legend\n",
    "\n",
    "\n",
    "    Plot average artifact for each channel, colored by lobe, \n",
    "    channels are split into 3 separate plots, based on their correlation with mean_rwave: equal number of channels in each group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    artif_per_ch : list\n",
    "        List of objects of class Avg_artif\n",
    "    tmin : float\n",
    "        Start time of the epoch (negative value)\n",
    "    tmax : float\n",
    "        End time of the epoch\n",
    "    m_or_g : str\n",
    "        Type of the channel: mag or grad\n",
    "    ecg_or_eog : str\n",
    "        Type of the artifact: ECG or EOG\n",
    "    chs_by_lobe : dict\n",
    "        Dictionary with channels split by lobe\n",
    "    flip_data : bool\n",
    "        Use True or False, doesnt matter here. It is only passed into the plotting function and influences the threshold presentation. But since treshold is not used in correlation method, this is not used.\n",
    "    verbose_plots : bool\n",
    "        If True, plots are shown in the notebook.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    artif_per_ch : list\n",
    "        List of objects of class Avg_artif\n",
    "    affected_derivs : list\n",
    "        List of objects of class QC_derivative (plots)\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ecg_or_eog = ecg_or_eog.lower()\n",
    "\n",
    "    df = pd.read_csv(f_path) #TODO: maybe remove reading csv and pass directly the df here?\n",
    "    df = df.drop(df[df['Type'] != m_or_g].index) #remove non needed channel kind\n",
    "\n",
    "\n",
    "    artif_time_vector = figure_x_axis(df, metric=ecg_or_eog)\n",
    "\n",
    "    most_correlated, middle_correlated, least_correlated, _, _, _ = split_correlated_artifacts_into_3_groups_csv(df, ecg_or_eog)\n",
    "\n",
    "    #plot using plotly: \n",
    "    # artif_per_ch.artif_data - a third of all channels that are the most correlated with mean_rwave, \n",
    "    # artif_per_ch.artif_data - a third of all channels that are the less with mean_rwave, \n",
    "    # artif_per_ch.artif_data - a third of all channels that are the least correlated with mean_rwave\n",
    "\n",
    "\n",
    "    fig_most_affected = plot_affected_channels_csv(most_correlated, None, artif_time_vector, m_or_g, fig_tit=ecg_or_eog+' most affected channels (smoothed): ', flip_data=flip_data, smoothed = True, verbose_plots=False)\n",
    "    fig_middle_affected = plot_affected_channels_csv(middle_correlated, None, artif_time_vector, m_or_g, fig_tit=ecg_or_eog+' middle affected channels (smoothed): ', flip_data=flip_data, smoothed = True, verbose_plots=False)\n",
    "    fig_least_affected = plot_affected_channels_csv(least_correlated, None, artif_time_vector, m_or_g, fig_tit=ecg_or_eog+' least affected channels (smoothed): ', flip_data=flip_data, smoothed = True, verbose_plots=False)\n",
    "\n",
    "    #set the same Y axis limits for all 3 figures for clear comparison:\n",
    "\n",
    "    if ecg_or_eog.lower() == 'ecg':\n",
    "            prefix = 'mean ECG_sec_'\n",
    "    elif ecg_or_eog.lower() == 'eog': \n",
    "            prefix = 'mean EOG_sec_'\n",
    "\n",
    "    cols = [column for column in df if column.startswith(prefix)]\n",
    "    cols = ['Name']+cols\n",
    "\n",
    "    temp_df = df[cols]\n",
    "\n",
    "    # print('________temp_df_______')\n",
    "    # print(temp_df)\n",
    "\n",
    "    ymax = temp_df.loc[:, temp_df.columns != 'Name'].max().max()\n",
    "    ymin = temp_df.loc[:, temp_df.columns != 'Name'].min().min()\n",
    "\n",
    "    print(ymax, ymin)\n",
    "    \n",
    "    # # combine the data lists into one numpy array\n",
    "    # arr = np.array([ch.artif_data for ch in artif_per_ch])\n",
    "\n",
    "    # # #find the highest and lowest value in artif_per_ch.artif_data:\n",
    "    # ymin = np.min(arr)\n",
    "    # ymax = np.max(arr)\n",
    "\n",
    "    ylim = [ymin*.95, ymax*1.05]\n",
    "\n",
    "    # update the layout of all three figures with the same y-axis limits\n",
    "    fig_most_affected.update_layout(yaxis_range=ylim)\n",
    "    fig_middle_affected.update_layout(yaxis_range=ylim)\n",
    "    fig_least_affected.update_layout(yaxis_range=ylim)\n",
    "\n",
    "    if verbose_plots is True:\n",
    "        fig_most_affected.show()\n",
    "        fig_middle_affected.show()\n",
    "        fig_least_affected.show()\n",
    "    \n",
    "    affected_derivs = []\n",
    "   \n",
    "    return affected_derivs\n",
    "\n",
    "\n",
    "m_or_g_chosen = ['mag']\n",
    "for m_or_g in m_or_g_chosen:\n",
    "        affected_derivs = plot_artif_per_ch_correlated_lobes_csv('/Volumes/M2_DATA/ECGs_by_lobe.csv', m_or_g, 'ECG', flip_data=False, verbose_plots=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotting_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/Paris2020/sub-emptyroom/meg/sub-emptyroom_task-Paris5_meg.fif', allow_maxshield=True)\n",
    "#raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/LeerraumMD2016/sub-emptyroom/meg/sub-emptyroom_task-Magdeburg2_meg.fif')\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aligned Wave Shapes:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Generate two aligned wave shapes\n",
    "time = np.linspace(0, 1, 100)\n",
    "wave1 = np.sin(2 * np.pi * 2 * time)\n",
    "wave2 = np.sin(2 * np.pi * 2 * time)\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = np.correlate(wave1, wave2, mode='same')\n",
    "corr1 = pearsonr(wave1, wave2)\n",
    "print(corr1)\n",
    "\n",
    "# Plot the wave shapes and correlation\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(time, wave1)\n",
    "plt.title('Wave 1')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(time, wave2)\n",
    "plt.title('Wave 2')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Misaligned Wave Shapes:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate two misaligned wave shapes\n",
    "time = np.linspace(0, 1, 100)\n",
    "wave1 = np.sin(2 * np.pi * 2 * time)\n",
    "wave2 = np.sin(2 * np.pi * 2 * (time + 0.15))  # Shifted by 0.2 seconds\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = np.correlate(wave1, wave2, mode='same')\n",
    "corr2 = pearsonr(wave1, wave2)\n",
    "print(corr2)\n",
    "\n",
    "# Plot the wave shapes and correlation\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(time, wave1)\n",
    "plt.title('Wave 1')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(time, wave2)\n",
    "plt.title('Wave 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create wave shapes\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of points in each array\n",
    "num_points = 100\n",
    "\n",
    "# Create an array of time values\n",
    "t = np.linspace(0, 2*np.pi, num_points)\n",
    "\n",
    "# Define the amplitudes for the R-wave shapes\n",
    "amplitudes = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5]\n",
    "\n",
    "# Define the maximum time shift in seconds\n",
    "max_shift = 0.4\n",
    "\n",
    "# Create five arrays with R-wave shapes, shifted in time\n",
    "waves = []\n",
    "for i, amplitude in enumerate(amplitudes):\n",
    "    # Generate a random time shift within the maximum shift range\n",
    "    time_shift = np.random.uniform(-max_shift, max_shift)\n",
    "    \n",
    "    # Shift the time values\n",
    "    shifted_t = t + time_shift\n",
    "    \n",
    "    # Create the R-wave shape with the shifted time values\n",
    "    wave = np.exp(-shifted_t) * np.sin(4*shifted_t) * amplitude\n",
    "    #waves.append(wave)\n",
    "    waves.append(wave[::-1])\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "for i, wave in enumerate(waves):\n",
    "    fig.add_trace(go.Scatter(x=time, y=wave, name=f'Wave {i+1}'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have two arrays: array1 and array2\n",
    "array1 = waves[0]\n",
    "array2 = waves[5]\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Assuming you have two arrays: array1 and array2\n",
    "\n",
    "# Find peaks in both arrays\n",
    "peaks1, _ = find_peaks(array1)\n",
    "peaks2, _ = find_peaks(array2)\n",
    "\n",
    "# Calculate the time shift based on the peak positions\n",
    "time_shift = peaks1[0] - peaks2[0]\n",
    "\n",
    "# Shift array2 to align with array1\n",
    "aligned_array2 = np.roll(array2, time_shift)\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the array1 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(array1)), y=array1, name='Array 1'))\n",
    "\n",
    "# Add the array2 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(array2)), y=array2, name='Array 2'))\n",
    "\n",
    "# Add the aligned_array2 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(aligned_array2)), y=aligned_array2, name='Aligned Array 2'))\n",
    "\n",
    "# Set the layout\n",
    "fig.update_layout(title='Aligned Arrays using Peak Detection',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='Amplitude')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "array1 = waves[0]\n",
    "array2 = -waves[5]\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the array1 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(array1)), y=array1, name='Array 1'))\n",
    "\n",
    "# Add the array2 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(array2)), y=array2, name='Array 2'))\n",
    "\n",
    "# Set the layout\n",
    "fig.update_layout(title='Aligned Arrays using Peak Detection',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='Amplitude')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming you have two arrays: array1 and array2\n",
    "\n",
    "# Find peaks in array1\n",
    "peaks1, _ = find_peaks(array1)\n",
    "\n",
    "# Initialize variables for best alignment\n",
    "best_time_shift = 0\n",
    "best_correlation = -np.inf\n",
    "best_aligned_array2 = None\n",
    "\n",
    "# Try aligning array2 in both orientations\n",
    "for flip in [False, True]:\n",
    "    # Flip array2 if needed\n",
    "    #aligned_array2 = np.flip(array2) if flip else array2\n",
    "    aligned_array2 = -array2 if flip else array2\n",
    "\n",
    "    # Find peaks in aligned_array2\n",
    "    peaks2, _ = find_peaks(aligned_array2)\n",
    "\n",
    "    # Calculate the time shift based on the peak positions\n",
    "    time_shift = peaks1[0] - peaks2[0]\n",
    "\n",
    "    # Shift aligned_array2 to align with array1\n",
    "    aligned_array2 = np.roll(aligned_array2, time_shift)\n",
    "\n",
    "    # Calculate the correlation between array1 and aligned_array2\n",
    "    correlation = np.corrcoef(array1, aligned_array2)[0, 1]\n",
    "\n",
    "    # Update the best alignment if the correlation is higher\n",
    "    if correlation > best_correlation:\n",
    "        best_correlation = correlation\n",
    "        best_time_shift = time_shift\n",
    "        best_aligned_array2 = aligned_array2\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the array1 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(array1)), y=array1, name='Array 1'))\n",
    "\n",
    "# Add the array2 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(array2)), y=array2, name='Array 2'))\n",
    "\n",
    "# Add the best_aligned_array2 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(best_aligned_array2)), y=best_aligned_array2, name='Aligned Array 2'))\n",
    "\n",
    "# Set the layout\n",
    "fig.update_layout(title='Aligned Arrays with Flipped Second Array',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='Amplitude')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "avg_ecg_epoch_data_nonflipped_limited_to_event = np.array(waves)\n",
    "\n",
    "max_values=np.max(np.abs(avg_ecg_epoch_data_nonflipped_limited_to_event), axis=1)\n",
    "print(max_values)\n",
    "max_values_ind=np.argsort(max_values)[::-1] \n",
    "print(max_values_ind)\n",
    "max_values_ind=max_values_ind[:5]\n",
    "\n",
    "chosen_5 = (avg_ecg_epoch_data_nonflipped_limited_to_event[max_values_ind])\n",
    "\n",
    "thresh_lvl_peakfinder = 5\n",
    "\n",
    "\n",
    "#get the highest peak for every channel:\n",
    "max_amplitude1 = []\n",
    "index_of_max_amplitude1=[]\n",
    "for ch_data in avg_ecg_epoch_data_nonflipped_limited_to_event:\n",
    "\n",
    "    thresh_mean=(max(ch_data) - min(ch_data)) / thresh_lvl_peakfinder\n",
    "    peak_locs_pos, _ = find_peaks(ch_data, prominence=thresh_mean)\n",
    "    peak_locs_neg, _ = find_peaks(-ch_data, prominence=thresh_mean)\n",
    "\n",
    "    all_peaks = np.concatenate((peak_locs_pos, peak_locs_neg))\n",
    "    print('all peaks', all_peaks)\n",
    "\n",
    "    #Find the peak with the maximal amplitude:\n",
    "\n",
    "    max_amplitude_peak = np.argmax(np.abs(ch_data[all_peaks]))\n",
    "\n",
    "    #now find the index of this point in the channel data:\n",
    "    index_of_max_amplitude1.append(all_peaks[max_amplitude_peak])\n",
    "\n",
    "    print('Index1', index_of_max_amplitude1)\n",
    "\n",
    "    #now find the magnitude of the data in this point:\n",
    "\n",
    "    max_amplitude1.append(ch_data[index_of_max_amplitude1[-1]])\n",
    "\n",
    "    \n",
    "\n",
    "# find 5 channels which have the highest peaks and get the locations of these peaks:\n",
    "highest_channels_sorted = np.argsort(max_amplitude1)[::-1] \n",
    "print(highest_channels_sorted)\n",
    "max_ind_of_chosen_5=highest_channels_sorted[:5]\n",
    "\n",
    "print(max_ind_of_chosen_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_max_amplitude2=[]\n",
    "for ch_data in max_ind_of_chosen_5:\n",
    "\n",
    "    thresh_mean=(max(ch_data) - min(ch_data)) / thresh_lvl_peakfinder\n",
    "    peak_locs_pos, _ = find_peaks(ch_data, prominence=thresh_mean)\n",
    "    peak_locs_neg, _ = find_peaks(-ch_data, prominence=thresh_mean)\n",
    "\n",
    "    all_peaks = np.concatenate((peak_locs_pos, peak_locs_neg))\n",
    "    print('all peaks', all_peaks)\n",
    "\n",
    "    #Find the peak with the maximal amplitude:\n",
    "\n",
    "    max_amplitude_peak = np.argmax(np.abs(ch_data[all_peaks]))\n",
    "\n",
    "\n",
    "    #6. Output the index of the point with the maximal amplitude:\n",
    "\n",
    "    index_of_max_amplitude1.append(all_peaks[max_amplitude_peak])\n",
    "    print('Index1', index_of_max_amplitude1)\n",
    "\n",
    "    if len(all_peaks)>1:\n",
    "        #7. Now find the second largest peak:\n",
    "        all_peaks_without_max = np.delete(all_peaks, max_amplitude_peak)\n",
    "\n",
    "        print('no max', all_peaks_without_max)\n",
    "\n",
    "        max_amplitude_peak = np.argmax(np.abs(ch_data[all_peaks_without_max]))\n",
    "\n",
    "\n",
    "        #6. Output the index of the point with the maximal amplitude:\n",
    "\n",
    "        index_of_max_amplitude2.append(all_peaks_without_max[max_amplitude_peak])\n",
    "        print('Index2', index_of_max_amplitude2)\n",
    "        \n",
    "    else:\n",
    "        index_of_max_amplitude2.append(np.nan)\n",
    "\n",
    "mean_index_of_max_amplitude1 = np.nanmean(index_of_max_amplitude1)\n",
    "\n",
    "# If in more than a half of cases there was no second biggest peak found, skip it and assign t) as first peak:\n",
    "non_zero_count = np.count_nonzero(index_of_max_amplitude2)\n",
    "percentage = (non_zero_count/len(index_of_max_amplitude2)) * 100\n",
    "\n",
    "if percentage < 50:\n",
    "    t0_peak = int(mean_index_of_max_amplitude1)\n",
    "else:\n",
    "    mean_index_of_max_amplitude2 = np.nanmean(index_of_max_amplitude2)\n",
    "    #Now out of them set the first peak (according to time) as t0.\n",
    "    t0_peak = int(np.nanmin([mean_index_of_max_amplitude1, mean_index_of_max_amplitude2]))\n",
    "\n",
    "\n",
    "print('mean_ind1', mean_index_of_max_amplitude1)\n",
    "print('mean_ind2', mean_index_of_max_amplitude2)\n",
    "\n",
    "\n",
    "print(t0_peak)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([5, 2, 9, 1, 7, 3])\n",
    "\n",
    "# Get the indices that would sort the array in ascending order\n",
    "sorted_indices = np.argsort(arr)\n",
    "\n",
    "# Index of the largest value\n",
    "largest_index = sorted_indices[-1]\n",
    "\n",
    "# Index of the second largest value\n",
    "second_largest_index = sorted_indices[-2]\n",
    "\n",
    "print(\"Index of the largest value:\", largest_index)\n",
    "print(\"Index of the second largest value:\", second_largest_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the MEG data\n",
    "raw=mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/openneuro/ds004107/sub-mind002/ses-01/meg/sub-mind002_ses-01_task-auditory_meg.fif', preload=True)\n",
    "\n",
    "display(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the EOG channel names\n",
    "eog_channels = ['EOG 061', 'EOG 062']\n",
    "\n",
    "# extract the data of 2 EOG channels\n",
    "eog_data = raw.copy().pick_channels(eog_channels).get_data()\n",
    "\n",
    "print(eog_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the data with plotly:\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "x_values = raw.times\n",
    "\n",
    "# Add a trace for the first subplot\n",
    "fig.add_trace(go.Scatter(x=x_values, y=eog_data[0], mode='lines', name='EOG 1'), row=1, col=1)\n",
    "\n",
    "# Add a trace for the second subplot\n",
    "fig.add_trace(go.Scatter(x=x_values, y=eog_data[1], mode='lines', name='EOG 2'), row=2, col=1)\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(title='EOG Data', xaxis_title='Time (s)', yaxis_title='Amplitude')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create two arrays\n",
    "array1 = np.array([1, 2, 3, 4, 5])\n",
    "array2 = np.array([6, 7, 8, 9, 10])\n",
    "\n",
    "# stack the arrays horizontally\n",
    "stacked = np.stack((array1, array2), axis=0)\n",
    "\n",
    "display(stacked)\n",
    "\n",
    "# calculate the covariance matrix\n",
    "covariance_matrix = np.cov(stacked)\n",
    "\n",
    "print(covariance_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Load the MEG data\n",
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/openneuro/ds003703/sub-a68d5xp5/meg/sub-a68d5xp5_task-listeningToSpeech_run-01_meg.fif')\n",
    "#raw=mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/openneuro/ds004107/sub-mind002/ses-01/meg/sub-mind002_ses-01_task-auditory_meg.fif', preload=True)\n",
    "\n",
    "# Select the EOG channels\n",
    "eog_channels = mne.pick_types(raw.info, meg=False, eeg=False, stim=False, eog=True)\n",
    "\n",
    "# Get the names of the EOG channels\n",
    "eog_channel_names = [raw.ch_names[ch] for ch in eog_channels]\n",
    "\n",
    "print('EOG channel names:', eog_channel_names)\n",
    "\n",
    "eog_events = mne.preprocessing.find_eog_events(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "picks_ECG = mne.pick_types(raw.info, ecg=True)\n",
    "ecg_ch_name = [raw.info['chs'][name]['ch_name'] for name in picks_ECG]\n",
    "\n",
    "arr=raw.get_data(picks=ecg_ch_name)[0] \n",
    "height = np.mean(arr) + 1 * np.std(arr)\n",
    "fs=raw.info['sfreq']\n",
    "peaks, _ = find_peaks(arr, height=height, distance=round(0.5 * fs)) #assume there are no peaks within 0.5 seconds from each other.\n",
    "ecg_events = peaks/fs\n",
    "\n",
    "# Define the time window of interest\n",
    "time_window = [0.2, 0.2]  # in seconds\n",
    "tmin=-0.2\n",
    "tmax=0.2\n",
    "\n",
    "# Convert time window to samples\n",
    "sfreq = 1000  # sampling frequency of your data\n",
    "time_window_samples = np.round(np.array(time_window) * sfreq).astype(int)\n",
    "print('samples', time_window_samples)\n",
    "\n",
    "# Initialize an empty array to store the extracted epochs\n",
    "epochs = np.zeros((len(peaks), int((tmax-tmin)*sfreq)))\n",
    "\n",
    "print('HERE')\n",
    "print(arr)\n",
    "print(epochs)\n",
    "\n",
    "# Loop through each ECG event and extract the corresponding epoch\n",
    "for i, event in enumerate(peaks):\n",
    "    start = event - time_window_samples[0]\n",
    "    start = np.round(event + tmin*sfreq).astype(int)\n",
    "    end = event + time_window_samples[1]\n",
    "    end= np.round(event + tmax*sfreq).astype(int)\n",
    "    epochs[i, :] = arr[start:end]\n",
    "\n",
    "#average all epochs:\n",
    "avg_ecg=np.mean(epochs, axis=0)\n",
    "\n",
    "#print average ecg with plotly:\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "#create time vector based on time window and sampling frequency:\n",
    "times= np.arange(tmin, tmax, 1/sfreq)\n",
    "fig.add_trace(go.Scatter(x=times, y=avg_ecg, mode='lines', name='ECG'))\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detect the R-wave peaks in the filtered ECG channel data\n",
    "r_peaks, ch_ecg, pulse, ecg_data_rec = mne.preprocessing.find_ecg_events(raw, return_ecg=True)\n",
    "print(ecg_data_rec)\n",
    "\n",
    "#plot the ECG data with plotly:\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "times=[t for t in range(len(ecg_data_rec[0]))]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=times, y=ecg_data_rec[0], mode='lines', name='ECG'))\n",
    "fig.update_layout(title='ECG data', xaxis_title='Time (s)', yaxis_title='ECG (mV)')\n",
    "fig.show()\n",
    "\n",
    "# Calculate the time difference between each R-wave peak and the first R-wave peak\n",
    "r_wave_epochs = (r_peaks - r_peaks[0]) / raw.info['sfreq']\n",
    "print('r_wave_epochs', r_wave_epochs)\n",
    "\n",
    "# Calculate the average R-wave epoch\n",
    "avg_r_wave_epoch = np.mean(r_wave_epochs)\n",
    "print('avg_r_wave_epoch', avg_r_wave_epoch)\n",
    "\n",
    "if ecg_ch_name:\n",
    "    # Extract the ECG channel data\n",
    "    ecg_data, times = raw.get_data(ecg_ch_name, return_times=True)\n",
    "    ecg_data2=ecg_data_rec\n",
    "else:\n",
    "    ecg_data=ecg_data_rec\n",
    "\n",
    "\n",
    "\n",
    "# Use the average R-wave epoch to extract a segment of data from the ECG channel\n",
    "avg_r_wave_data = ecg_data[:, int(avg_r_wave_epoch * raw.info['sfreq']) : int((avg_r_wave_epoch + 0.2) * raw.info['sfreq'])]\n",
    "avg_r_wave_data2 = ecg_data2[:, int(avg_r_wave_epoch * raw.info['sfreq']) : int((avg_r_wave_epoch + 0.2) * raw.info['sfreq'])]\n",
    "\n",
    "#plot the average R-wave epoch with plotly:\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "times=[t/raw.info['sfreq'] for t in range(len(avg_r_wave_data[0]))]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=times, y=avg_r_wave_data[0], mode='lines', name='ECG'))\n",
    "fig.add_trace(go.Scatter(x=times, y=avg_r_wave_data2[0], mode='lines', name='ECG2'))\n",
    "fig.update_layout(title='Average R-wave epoch', xaxis_title='Time (s)', yaxis_title='ECG (mV)')\n",
    "fig.show()\n",
    "\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([t/raw.info['sfreq'] for t in range(len(avg_r_wave_data[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data\n",
    "avg_r_wave_epoch * raw.info['sfreq']\n",
    "r_peaks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_r_wave_data = ecg_data[:, int(avg_r_wave_epoch * raw.info['sfreq']) : int((avg_r_wave_epoch + 0.2) * raw.info['sfreq'])]\n",
    "avg_r_wave_data2 = ecg_data2[:, int(avg_r_wave_epoch * raw.info['sfreq']) : int((avg_r_wave_epoch + 0.2) * raw.info['sfreq'])]\n",
    "\n",
    "#plot the average R-wave epoch with plotly:\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "times=[t/raw.info['sfreq'] for t in range(len(avg_r_wave_data[0]))]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=times, y=avg_r_wave_data[0], mode='lines', name='ECG'))\n",
    "fig.add_trace(go.Scatter(x=times, y=avg_r_wave_data2[0], mode='lines', name='ECG2'))\n",
    "fig.update_layout(title='Average R-wave epoch', xaxis_title='Time (s)', yaxis_title='ECG (mV)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Generate two waves\n",
    "wave1 = np.array([1, 2, 3, 4, 5])\n",
    "wave2 = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Calculate the Pearson correlation coefficient and p-value\n",
    "corr_coef, p_value = pearsonr(wave1, wave2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Pearson correlation coefficient:\", corr_coef)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "\n",
    "#plot both waves with plotly:\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=wave1, mode='lines', name='wave1'))\n",
    "fig.add_trace(go.Scatter(y=wave2, mode='lines', name='wave2'))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "lobe_colors = {\n",
    "        'Left Frontal': '#1f77b4',\n",
    "        'Right Frontal': '#ff7f0e',\n",
    "        'Left Temporal': '#2ca02c',\n",
    "        'Right Temporal': '#9467bd',\n",
    "        'Left Parietal': '#e377c2',\n",
    "        'Right Parietal': '#d62728',\n",
    "        'Left Occipital': '#bcbd22',\n",
    "        'Right Occipital': '#17becf'}\n",
    "\n",
    "print(random.choice(list(lobe_colors.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "import numpy as np\n",
    "\n",
    "# Generate some noisy wave data\n",
    "x = np.linspace(0, 2*np.pi, 100)\n",
    "y = np.sin(x) + np.random.normal(0, 0.5, 100)\n",
    "\n",
    "# Apply Gaussian smoothing with a sigma of 2\n",
    "y_smooth = gaussian_filter(y, sigma=4)\n",
    "\n",
    "# Plot the original and smoothed waves\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, label='Noisy wave')\n",
    "plt.plot(x, y_smooth, label='Smoothed wave')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "# create sample data\n",
    "df = pd.DataFrame({'values': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, index=['A']*10)\n",
    "df = df.T\n",
    "print(df)\n",
    "\n",
    "# create box plot trace\n",
    "box_trace = go.Box(x=df.iloc[0], orientation='h')\n",
    "#box_trace = go.Box(x=df['values'], y=df.index, orientation='h', name='')\n",
    "\n",
    "fig = go.Figure(data=box_trace)\n",
    "\n",
    "for col in df.columns:\n",
    "    fig.add_trace(go.Scatter(x=df[col], name=col))\n",
    "\n",
    "# for v in df['values']:\n",
    "#     #fig.add_trace(go.Scatter(x=df['values'], y=df.index, mode='markers', marker=dict(size=5, color='yellow'), name='Scatter Plot', hovertext=df.index))\n",
    "#     fig.add_trace(go.Scatter(x=[v], y=['A'], mode='markers', marker=dict(size=5, color='yellow'), name='Scatter Plot', hovertext=df.index))\n",
    "\n",
    "# plot figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add box plot trace\n",
    "fig.add_trace(go.Box(x=[1, 2, 3, 4, 5]))\n",
    "\n",
    "# Add horizontal line at y=0\n",
    "fig.update_layout(\n",
    "    shapes=[\n",
    "        dict(\n",
    "            type='line',\n",
    "            yref='y',\n",
    "            y0=0,\n",
    "            y1=0,\n",
    "            xref='paper',\n",
    "            x0=0,\n",
    "            x1=1\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create example dataset\n",
    "np.random.seed(123)\n",
    "std_val = pd.DataFrame({'Group': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
    "                     'Value': np.random.normal(size=6)})\n",
    "\n",
    "# Create box plot with custom marker colors\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(x=std_val['Group'], y=std_val['Value'], name='Value',\n",
    "                     marker=dict(color='red')))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Box plot with custom marker colors',\n",
    "                  xaxis_title='Group', yaxis_title='Value')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# create a box plot with custom marker color\n",
    "trace = go.Box(\n",
    "    y=[1, 2, 3, 4, 5],\n",
    "    marker=dict(\n",
    "        color='blue'\n",
    "    )\n",
    ")\n",
    "\n",
    "# create a figure and add the box plot trace\n",
    "fig = go.Figure(data=[trace])\n",
    "\n",
    "# show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This here is to save all the average ECG/EOG data into a pickle file, so I can test difefrent wave detection algorythms on them without running the pipeline again\n",
    "\n",
    "import pickle \n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# open a file in write binary mode\n",
    "with open(\"avg_ecg.pkl\", \"wb\") as f:\n",
    "    # dump the list of objects into the file\n",
    "    pickle.dump(avg_ecg, f)\n",
    "\n",
    "with open(\"avg_eog.pkl\", \"wb\") as f:\n",
    "    # dump the list of objects into the file\n",
    "    pickle.dump(avg_eog, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This here is to open the pickle files from above and plot the data\n",
    "\n",
    "import pickle \n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# open the file in read binary mode\n",
    "with open(\"avg_ecg0.pkl\", \"rb\") as f:\n",
    "    # load the list of objects from the file\n",
    "    eog_list = pickle.load(f)\n",
    "\n",
    "print(eog_list[0])\n",
    "\n",
    "sfreq=1000\n",
    "t = np.round(np.arange(-0.4, 0.4+1/sfreq, 1/sfreq), 3) #yes, you need to round\n",
    "fig0=go.Figure()\n",
    "for x in range(0, len(eog_list)):\n",
    "    fig_temp=eog_list[x].plot_epoch_and_peak(t, 'Channels affected by ECG artifact: ', 'mag', fig0)\n",
    "    for trace in fig_temp['data']:\n",
    "        fig0.add_trace(trace)\n",
    "\n",
    "fig0.update_layout(\n",
    "    yaxis = dict(\n",
    "            showexponent = 'all',\n",
    "            exponentformat = 'e')) \n",
    "fig0.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for EOG\n",
    "\n",
    "import pickle \n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "# open the file in read binary mode\n",
    "with open(\"avg_eog0.pkl\", \"rb\") as f:\n",
    "    # load the list of objects from the file\n",
    "    eog_list = pickle.load(f)\n",
    "\n",
    "sfreq=1000\n",
    "t = np.round(np.arange(-0.4, 0.4+1/sfreq, 1/sfreq), 3) #yes, you need to round\n",
    "fig0=go.Figure()\n",
    "for x in range(0, len(eog_list)):\n",
    "    fig_temp=eog_list[x].plot_epoch_and_peak(t, 'Channels affected by ECG artifact: ', 'mag')\n",
    "    for trace in fig_temp['data']:\n",
    "        fig0.add_trace(trace)\n",
    "\n",
    "fig0.update_layout(\n",
    "    yaxis = dict(\n",
    "            showexponent = 'all',\n",
    "            exponentformat = 'e')) \n",
    "fig0.show()\n",
    "\n",
    "#Now apply the gaussia filter to each trace and plot result in the same figure:\n",
    "fig0_new=deepcopy(fig0)\n",
    "for trace in fig0_new['data']:\n",
    "    y=trace['y']\n",
    "    y_smooth = gaussian_filter(y, sigma=10)\n",
    "    trace['y']=y_smooth\n",
    "\n",
    "fig0_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/LeerraumAarhus2017/sub-emptyroom/meg/sub-emptyroom_task-Aarhus_meg.fif', preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show sensor posiions using mne:\n",
    "\n",
    "import mne\n",
    "\n",
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/LeerraumAarhus2017/sub-emptyroom/meg/sub-emptyroom_task-Aarhus_meg.fif', preload=True)\n",
    "\n",
    "mne.viz.plot_sensors(raw.info, kind='topomap', ch_type='grad', show_names=True, ch_groups='position')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT SENSORS IN 2D with plotly\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import mne\n",
    "\n",
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/LeerraumAarhus2017/sub-emptyroom/meg/sub-emptyroom_task-Aarhus_meg.fif', preload=True)\n",
    "\n",
    "\n",
    "mag_ch_names = raw.copy().pick_types(meg='mag').ch_names if 'mag' in raw else None\n",
    "grad_ch_names = raw.copy().pick_types(meg='grad').ch_names if 'grad' in raw else None\n",
    "channels_objs = {'mag': mag_ch_names, 'grad': grad_ch_names}\n",
    "\n",
    "# Get the sensor locations\n",
    "sensor_locs = raw.info['chs']\n",
    "#print(sensor_locs)\n",
    "#coords_mag=[loc['loc'][:2] for loc in sensor_locs]\n",
    "coords_mag=[loc['loc'] for loc in sensor_locs if loc['ch_name'] in mag_ch_names]\n",
    "#print(len(coords), coords)\n",
    "print(len(coords_mag), coords_mag)\n",
    "\n",
    "x = [r[0] for r in coords_mag]\n",
    "y = [r[1] for r in coords_mag]\n",
    "#x, y, z = [loc['loc'][:3] for loc in sensor_locs]\n",
    "names = [loc['ch_name'] for loc in sensor_locs if loc['ch_name'] in mag_ch_names]\n",
    "kinds= [loc['kind'] for loc in sensor_locs]\n",
    "print(kinds)\n",
    "\n",
    "# Create a scatter plot of the sensor locations\n",
    "fig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', text=names))\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=1000)\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "fig.update_layout(title='MEG Sensor Locations', xaxis_title='X', yaxis_title='Y')\n",
    "\n",
    "# Add a circle shape to the plot to show the position of the head\n",
    "fig.update_layout(\n",
    "    shapes=[\n",
    "        dict(\n",
    "            type='circle',\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            x0=-0.1,\n",
    "            y0=-0.1,\n",
    "            x1=0.1,\n",
    "            y1=0.12,\n",
    "            line=dict(color='red', width=2),\n",
    "            opacity=0.5\n",
    "        ),\n",
    "        dict(\n",
    "            type='line',\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            x0=[0, -0.02],\n",
    "            y0=[0.1, 0.08],\n",
    "            line=dict(color='black', width=2)\n",
    "        ),\n",
    "        dict(\n",
    "            type='line',\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            x0=[0, 0.02],\n",
    "            y0=[0.1, 0.08],\n",
    "            line=dict(color='black', width=2)\n",
    "        ),\n",
    "        dict(\n",
    "            type='line',\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            x0=[-0.02, 0.02],\n",
    "            y0=[0.08, 0.08],\n",
    "            line=dict(color='black', width=2))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Show the plot \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/LeerraumAarhus2017/sub-emptyroom/meg/sub-emptyroom_task-Aarhus_meg.fif', preload=True)\n",
    "\n",
    "#PLOT 3 D\n",
    "\n",
    "def switch_names_on_off(fig):\n",
    "    # Define the buttons\n",
    "    buttons = [\n",
    "    dict(label='Show channel names when hovering',\n",
    "         method='update',\n",
    "         args=[{'mode': 'markers'}]),\n",
    "    dict(label='Always show channel names',\n",
    "         method='update',\n",
    "         args=[{'mode': 'markers+text'}])\n",
    "    ]\n",
    "\n",
    "    # Add the buttons to the layout\n",
    "    fig.update_layout(updatemenus=[dict(type='buttons',\n",
    "                                        showactive=True,\n",
    "                                        buttons=buttons)])\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "# Extract the sensor locations and names for magnetometers\n",
    "mag_locs = raw.copy().pick_types(meg='mag').info['chs']\n",
    "mag_pos = [ch['loc'][:3] for ch in mag_locs]\n",
    "mag_names = [ch['ch_name'] for ch in mag_locs]\n",
    "\n",
    "# Create the magnetometer plot with markers only\n",
    "\n",
    "mag_fig = go.Figure(data=[go.Scatter3d(x=[pos[0] for pos in mag_pos],\n",
    "                                       y=[pos[1] for pos in mag_pos],\n",
    "                                       z=[pos[2] for pos in mag_pos],\n",
    "                                       mode='markers',\n",
    "                                       marker=dict(size=5),\n",
    "                                       text=mag_names,\n",
    "                                       hovertemplate='%{text}')],\n",
    "                                       layout=go.Layout(width=1000, height=1000))\n",
    "\n",
    "mag_fig.update_layout(title='Magnetometers')\n",
    "\n",
    "mag_fig = switch_names_on_off(mag_fig)\n",
    "mag_fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# Extract the sensor locations and names for gradiometers\n",
    "grad_locs = raw.copy().pick_types(meg='grad').info['chs']\n",
    "grad_pos = [ch['loc'][:3] for ch in grad_locs]\n",
    "grad_names = [ch['ch_name'] for ch in grad_locs]\n",
    "\n",
    "#since grads have 2 sensors located in the same spot - need to put their names together to make pretty plot labels:\n",
    "\n",
    "grad_pos_together = []\n",
    "grad_names_together = []\n",
    "\n",
    "for i in range(len(grad_pos)-1):\n",
    "    if all(x == y for x, y in zip(grad_pos[i], grad_pos[i+1])):\n",
    "        grad_pos_together += [grad_pos[i]]\n",
    "        grad_names_together += [grad_names[i]+', '+grad_names[i+1]]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Add both sets of gradiometer positions to the plot:\n",
    "grad_fig = go.Figure(data=[go.Scatter3d(x=[pos[0] for pos in grad_pos_together],\n",
    "                                        y=[pos[1] for pos in grad_pos_together],\n",
    "                                        z=[pos[2] for pos in grad_pos_together],\n",
    "                                        mode='markers',\n",
    "                                        marker=dict(size=5),\n",
    "                                        text=grad_names_together,\n",
    "                                        hovertemplate='%{text}')],\n",
    "                                        layout=go.Layout(width=1000, height=1000))\n",
    "\n",
    "grad_fig.update_layout(title='Gradiometers')\n",
    "\n",
    "\n",
    "# Add the button to have names show up on hover or always:\n",
    "grad_fig = switch_names_on_off(grad_fig)\n",
    "\n",
    "# Show the plots\n",
    "\n",
    "grad_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MUSCLE ARIFACTS IN EMPTYROOM DATA:\n",
    "# Discussed with Andreas:\n",
    "# We can see very high muscle scores at the very beginning and end of the empty room recording\n",
    "# Are these real muscle artifacts or filtering errors?\n",
    "# Cut out 1st second of the data where they are visible.\n",
    "# make Fourier transform of the 1st second and see if there are high amplitudes visible for the muscle frequencies - nope \n",
    "# most likely this is filtering.\n",
    "# next, follow the MNE steps for muscle artifact detection: they use first filtering at 11--140 hz, then Hilbert\n",
    "# plotted raw data after the applied filter, and Hilbert - see cut artifacts in the beginning and end. (WJY arw we sure it  s not hilbert?)\n",
    "# then, tried to only filter - very noisy data. but most likely the filtering is the source. Because of the cut-off in the beginning and end.\n",
    "# Solutions: zero padding in the beginning and end before filtering, which will be cut off after. But may still create a jump while filtering and keep the artifact.\n",
    "# Better: add 2s of dummy data at the beginning and end of the recording, and then crop it out (the data added should be mirrored). This will not create a jump in the filtering.\n",
    "# Tried\n",
    "\n",
    "# Problem found! 2 problems: \n",
    "# 1st: The main artifact is actually introduced by filtering power lines. filtering the data at 150 Hz (harmonics) clearly creates this artifact.\n",
    "# Removed that and any other filtering over the range of muscle freqs, since we don't need them anyways. (over 140 Hz)\n",
    "# 2nd: Still some artifact is present in the beginning and end of the recording. For this attach mirrored data on both ends.\n",
    "# Then detect muscle, then cut the resulting scores away for the attached period.\n",
    "# There will be still some very minimal artifact at the beginning/end of this attachment - probably due to the attachment itself: the mirrored data is still not a normally shaped signal.\n",
    "# See example in cell above of how all 3 option look: oroginal, with attached data and with attached and cut away.\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import mne\n",
    "\n",
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/LeerraumAarhus2017/sub-emptyroom/meg/sub-emptyroom_task-Aarhus_meg.fif', preload=True)\n",
    "\n",
    "#%matplotlib qt\n",
    "raw_first = raw.copy().crop(tmin=0, tmax=1)\n",
    "#raw_first.plot()\n",
    "\n",
    "\n",
    "std_val=raw_first.get_data()\n",
    "\n",
    "window = np.hanning(std_val.shape[-1])*std_val\n",
    "\n",
    "window\n",
    "\n",
    "freqs = np.fft.rfftfreq(window.shape[-1], 1/raw.info['sfreq'])\n",
    "\n",
    "components = np.fft.fft(window, axis=-1)\n",
    "\n",
    "components.shape\n",
    "\n",
    "fig = go.Figure()\n",
    "for ch in range(15, 250):\n",
    "    fig.add_trace(go.Scatter(x=freqs, y = np.abs(components[ch, 0:500])))\n",
    "fig.add_trace(go.Scatter(x=freqs, y = np.abs(components[0, 0:500])))\n",
    "#from mne annotate_muscle_zscore:\n",
    "from scipy.stats import zscore\n",
    "from scipy.ndimage import label\n",
    "\n",
    "filter_freq=(110, 140)\n",
    "legend_category = 'mag'\n",
    "\n",
    "raw_copy = raw_first.copy()\n",
    "raw_copy.load_data()\n",
    "\n",
    "if legend_category is None:\n",
    "    raw_ch_type = raw_copy.get_channel_types()\n",
    "    if 'mag' in raw_ch_type:\n",
    "        legend_category = 'mag'\n",
    "    elif 'grad' in raw_ch_type:\n",
    "        legend_category = 'grad'\n",
    "    elif 'eeg' in raw_ch_type:\n",
    "        legend_category = 'eeg'\n",
    "    else:\n",
    "        raise ValueError('No M/EEG channel types found, please specify a'\n",
    "                            ' ch_type or provide M/EEG sensor data')\n",
    "\n",
    "if legend_category in ('mag', 'grad'):\n",
    "    raw_copy.pick_types(meg=legend_category, ref_meg=False)\n",
    "else:\n",
    "    legend_category = {'meg': False, legend_category: True}\n",
    "    raw_copy.pick_types(**legend_category)\n",
    "\n",
    "#raw_copy.filter(filter_freq[0], filter_freq[1], fir_design='firwin',\n",
    "#                pad=\"reflect_limited\")\n",
    "\n",
    "hilb_applied=raw_copy.apply_hilbert(envelope=True)\n",
    "hilb_applied.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the list of values into a NumPy array\n",
    "values = np.array([1, 2, 1.8, 2.5, 3, 3.5, 4, 3.8, 5, 4, 2, 2.1, 1, 0, 2, 4, 6])\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt, savgol_filter, find_peaks\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Generate a noisy wave shape\n",
    "t = np.linspace(0, 10, 1000)\n",
    "y = np.sin(t) + 0.5*np.random.randn(len(t))\n",
    "\n",
    "data = np.random.randn(1000) #no wave shape\n",
    "# Load the noisy wave data into a NumPy array\n",
    "wave_data = data\n",
    "\n",
    "# Create a Plotly figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t, y=wave_data, mode='lines', name='Noisy Wave'))\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Amplitude', title='Noisy Wave Shape')\n",
    "fig.show()\n",
    "\n",
    "# Apply a low-pass filter to remove high-frequency noise\n",
    "b, a = butter(5, 0.1, 'low')\n",
    "filtered_data = filtfilt(b, a, wave_data)\n",
    "\n",
    "# plot filtered data\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t, y=filtered_data, mode='lines', name='Filtered Wave'))\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Amplitude', title='Filtered Wave Shape')\n",
    "fig.show()\n",
    "\n",
    "# Apply a Savitzky-Golay filter to further reduce noise and extract the underlying wave shape\n",
    "#smoothed_data = savgol_filter(wave_data, window_length=int(len(wave_data)/4), polyorder=3)\n",
    "smoothed_data = savgol_filter(data, window_length=100, polyorder=3)\n",
    "\n",
    "# Identify the shape of the wave using peak detection or curve fitting\n",
    "# For example, you can use the `scipy.signal.find_peaks` function to detect peaks in the smoothed data\n",
    "#peaks, _ = find_peaks(smoothed_data, height=0.5*np.max(smoothed_data))\n",
    "peaks, _ = find_peaks(smoothed_data)\n",
    "\n",
    "# plot smoothed data\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t, y=smoothed_data, mode='lines', name='Smoothed Wave'))\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Amplitude', title='Smoothed Wave Shape')\n",
    "fig.add_trace(go.Scatter(x=t[peaks], y=smoothed_data[peaks], mode='markers', name='Peaks'))\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Generate a noisy wave shape\n",
    "t = np.linspace(0, 10, 1000)\n",
    "y = np.sin(t) + 0.5*np.random.randn(len(t))\n",
    "\n",
    "# Find the peaks in the wave\n",
    "peaks, _ = find_peaks(y)\n",
    "\n",
    "# Count the number of peaks\n",
    "num_peaks = len(peaks)\n",
    "\n",
    "# Create a Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the noisy wave shape to the figure\n",
    "fig.add_trace(go.Scatter(x=t, y=y, mode='lines', name='Noisy Wave'))\n",
    "\n",
    "# Add the peaks to the figure\n",
    "fig.add_trace(go.Scatter(x=t[peaks], y=y[peaks], mode='markers', name='Peaks'))\n",
    "\n",
    "# Add axis labels and a title\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Amplitude', title='Noisy Wave Shape')\n",
    "\n",
    "# Show the figure and print the number of peaks\n",
    "fig.show()\n",
    "print(f'The wave has {num_peaks} crest(s).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "picks_EOG = mne.pick_types(raw.info, eog=True)\n",
    "eog_ch_name = [raw.info['chs'][name]['ch_name'] for name in picks_EOG]\n",
    "eog_ch_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_psd = [2.93686870e-12, 5.37336497e-13, 2.34749324e-13, 1.70403629e-13\n",
    ", 1.42868936e-13, 1.10614848e-13, 1.01586902e-13, 9.41699507e-14\n",
    ", 8.41904711e-14, 7.56254639e-14, 6.98933286e-14, 6.47116338e-14\n",
    ", 5.37107007e-14, 5.42174045e-14, 4.78692577e-14, 4.36476164e-14\n",
    ", 3.69272073e-14, 3.81479068e-14, 4.07614720e-14, 3.71683505e-14\n",
    ", 3.74843265e-14, 3.57210926e-14, 3.99173535e-14, 4.87143053e-14\n",
    ", 4.20066645e-14, 3.84896719e-14, 3.13482998e-14, 2.86289627e-14\n",
    ", 2.82586165e-14, 2.71780036e-14, 2.48692250e-14, 2.71251350e-14\n",
    ", 2.79561808e-14, 2.72047767e-14, 2.79637330e-14, 2.55955578e-14\n",
    ", 2.53291180e-14, 2.01500680e-14, 2.12080778e-14, 2.18529602e-14\n",
    ", 2.12775368e-14, 2.14334140e-14, 2.18751322e-14, 1.97884378e-14\n",
    ", 1.89952388e-14, 1.79404586e-14, 2.01555022e-14, 2.21105500e-14\n",
    ", 1.87897255e-14, 1.95585625e-14, 2.07067862e-14, 2.15786185e-14\n",
    ", 1.78539522e-14, 1.89461927e-14, 1.83714513e-14, 1.91272285e-14\n",
    ", 1.81790918e-14, 1.51935998e-14, 1.55331746e-14, 1.37627320e-14\n",
    ", 1.37333973e-14, 1.47261781e-14, 1.35323358e-14, 1.23234074e-14\n",
    ", 1.26296023e-14, 1.39794705e-14, 1.32391885e-14, 1.33172509e-14\n",
    ", 1.40752537e-14, 1.35291881e-14, 1.46771014e-14, 1.57039580e-14\n",
    ", 1.76870590e-14, 2.11409680e-14, 2.66470647e-14, 2.09066429e-14\n",
    ", 1.68226688e-14, 1.63034232e-14, 1.32317697e-14, 1.20372472e-14\n",
    ", 1.10395275e-14, 1.17336558e-14, 1.12817157e-14, 1.31068881e-14\n",
    ", 1.36940739e-14, 1.48016686e-14, 1.35999052e-14, 1.56644411e-14\n",
    ", 1.51726149e-14, 1.95274934e-14, 1.84669709e-14, 1.89443054e-14\n",
    ", 1.82544652e-14, 1.92617658e-14, 1.80902967e-14, 2.17239287e-14\n",
    ", 2.67917043e-14, 4.45194367e-14, 2.01857457e-12, 4.40594585e-12\n",
    ", 1.74602155e-12, 4.30562941e-14, 2.53461498e-14, 1.87278757e-14\n",
    ", 1.54232144e-14, 1.67924147e-14, 1.22749773e-14, 1.25017017e-14\n",
    ", 1.22475994e-14, 1.02921463e-14, 1.07700101e-14, 1.02658035e-14\n",
    ", 9.54949869e-15, 9.84280694e-15, 8.88745310e-15, 9.02206922e-15\n",
    ", 8.47210049e-15, 8.64491709e-15, 1.32254861e-14, 1.89573615e-14\n",
    ", 1.23079196e-14, 8.62994931e-15, 8.12535185e-15, 8.01035318e-15\n",
    ", 7.53220890e-15, 8.02056256e-15, 7.90231409e-15, 7.63270083e-15\n",
    ", 7.93212379e-15, 7.28368608e-15, 7.59772607e-15, 7.26136429e-15\n",
    ", 7.86504197e-15, 7.36256360e-15, 7.02847343e-15, 7.08620266e-15\n",
    ", 6.86068169e-15, 7.21792433e-15, 7.28674098e-15, 6.88181371e-15\n",
    ", 6.78751472e-15, 6.59002904e-15, 6.74850515e-15, 6.53743454e-15\n",
    ", 6.70834535e-15, 6.72520433e-15, 6.78371437e-15, 6.70420118e-15\n",
    ", 6.97442862e-15, 7.26767528e-15, 6.79388360e-15, 6.83101277e-15\n",
    ", 6.90684197e-15, 6.45716620e-15, 6.66190889e-15, 6.49304182e-15\n",
    ", 6.38068712e-15, 6.29160702e-15, 5.92354089e-15, 6.33890242e-15\n",
    ", 6.33787606e-15, 5.76688121e-15, 6.31821916e-15, 6.34536916e-15\n",
    ", 6.51250512e-15, 6.43164190e-15, 6.46530769e-15, 6.44724883e-15\n",
    ", 7.48305304e-15, 7.50925230e-15, 6.28419317e-15, 6.33908319e-15\n",
    ", 5.86954984e-15, 6.54561206e-15, 6.08872456e-15, 6.40874736e-15\n",
    ", 5.95870142e-15, 6.13488554e-15, 5.83721527e-15, 5.87793931e-15\n",
    ", 5.81207088e-15, 5.98748087e-15, 5.94551525e-15, 5.75415575e-15\n",
    ", 5.66968278e-15, 6.11006036e-15, 5.72066372e-15, 5.96629716e-15\n",
    ", 5.80372053e-15, 5.75583336e-15, 5.84628922e-15, 5.63642362e-15\n",
    ", 5.34942930e-15, 5.75920960e-15, 6.05337029e-15, 6.61372576e-15\n",
    ", 7.14210116e-15, 6.94968538e-15, 1.85742697e-14, 3.52090532e-14\n",
    ", 1.48785528e-14, 6.23577963e-15, 5.66229647e-15, 5.39212323e-15\n",
    ", 5.32890121e-15, 5.54967559e-15, 5.29485491e-15, 5.65665900e-15\n",
    ", 5.31337965e-15, 5.30139224e-15, 5.21434237e-15, 5.61739646e-15\n",
    ", 5.62673191e-15, 5.68441483e-15, 5.43332729e-15, 5.34563989e-15\n",
    ", 5.69510011e-15, 6.43038706e-15, 5.52069097e-15, 5.22891308e-15\n",
    ", 5.06513758e-15, 5.15715319e-15, 5.32484298e-15, 5.37071225e-15\n",
    ", 5.24099974e-15, 5.14413780e-15, 5.05322799e-15, 5.27277366e-15\n",
    ", 5.17209094e-15, 5.19895605e-15, 5.04662049e-15, 5.13492402e-15\n",
    ", 5.39573281e-15, 5.13639899e-15, 5.29696474e-15, 5.29749076e-15\n",
    ", 5.23187737e-15, 5.14179424e-15, 1.44011463e-14, 2.29753019e-14\n",
    ", 8.85041560e-15, 5.16398069e-15, 5.09075672e-15, 5.06681957e-15\n",
    ", 5.17284653e-15, 4.99688083e-15, 5.01988585e-15, 5.07952302e-15\n",
    ", 4.97188381e-15, 5.17733558e-15, 4.97124292e-15, 4.96910679e-15\n",
    ", 4.96283207e-15, 5.07193856e-15, 4.80712108e-15, 4.97630935e-15\n",
    ", 4.93727883e-15, 4.84091247e-15, 5.07370238e-15, 4.76459850e-15\n",
    ", 4.86678392e-15, 5.03907955e-15, 4.91645908e-15, 4.99691785e-15\n",
    ", 4.81326372e-15, 5.56398292e-15, 5.40286001e-15, 4.91762834e-15\n",
    ", 4.96042200e-15, 4.86125369e-15, 5.04306529e-15, 4.89229744e-15\n",
    ", 4.93924928e-15, 4.91889752e-15, 4.92336366e-15, 4.91476256e-15\n",
    ", 4.90731383e-15, 4.79751988e-15, 4.96181659e-15, 5.04353794e-15]\n",
    "\n",
    "#plot the data with ploty:\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "freqs = [i/2 for i in range(0, 280)]\n",
    "prominence_lvl_pos = 50\n",
    "prominence_pos=(max(one_psd) - min(one_psd)) / prominence_lvl_pos\n",
    "noisy_freqs_indexes, _ = find_peaks(one_psd, prominence=prominence_pos)\n",
    "noisy_freqs_indexes = [int(i) for i in noisy_freqs_indexes]\n",
    "\n",
    "for i in range(0,2):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=freqs, y=one_psd, name='psd'))\n",
    "    fig.update_layout(title=' PSD', xaxis_title='Frequency', yaxis_title='Amplitude',\n",
    "            yaxis = dict(\n",
    "            showexponent = 'all',\n",
    "            exponentformat = 'e'))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=[freqs[noisy_freqs_indexes[0]]], y=[one_psd[noisy_freqs_indexes[0]]], mode='markers', name='peaks'))\n",
    "\n",
    "    if i == 0:\n",
    "        fig.update_yaxes(type=\"log\")\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_head_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool('False')\n",
    "\n",
    "#convert \"false\" to boolean:\n",
    "import ast\n",
    "t = ast.literal_eval('False')\n",
    "t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne-qc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
