{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path of the parent directory of the current script\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "from meg_qc.calculation.meg_qc_pipeline import make_derivative_meg_qc\n",
    "\n",
    "config_file_path = parent_dir+'/meg_qc/settings/settings.ini' \n",
    "internal_config_file_path=parent_dir+'/meg_qc/settings/settings_internal.ini' # internal settings in in\n",
    "#raw, raw_cropped_filtered_resampled, QC_derivs, QC_simple, df_head_pos, head_pos, scores_muscle_all1, scores_muscle_all2, scores_muscle_all3, raw1, raw2, raw3, avg_ecg, avg_eog = make_derivative_meg_qc(config_file_path, internal_config_file_path)\n",
    "\n",
    "for_report = make_derivative_meg_qc(config_file_path, internal_config_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities =  {'subject': {'009'}, 'session': {'1'}, 'task': {'deduction'}, 'run': {1}, 'description': {'STDs', 'Muscle', 'ECGchannel', 'PSDwavesGrad', 'PtPsManual', 'Sensors', 'SimpleMetrics', 'RawInfo', 'PSDs', 'ReportStrings', 'PSDnoiseGrad', 'PSDwavesMag', 'ECGs', 'EOGchannel', 'PSDnoiseMag', 'EOGs'}}\n",
    "print('entities: ', entities)\n",
    "\n",
    "\n",
    "def modify_entity_name(entities):\n",
    "\n",
    "    \"\"\"\n",
    "    Modify the names of the entities to be comply with ancpbids.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    entities : dict\n",
    "        A dictionary of entities and their subcategories.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary of entities and their subcategories with modified names\n",
    "    \"\"\"\n",
    "\n",
    "    print('___MEGqc___: ', 'Entities found in the dataset: ', entities)\n",
    "\n",
    "    #old_new_categories = {'description': 'METRIC', 'subject': 'SUBJECT', 'session': 'SESSION', 'task': 'TASK', 'run': 'RUN'}\n",
    "\n",
    "    old_new_categories = {'description': 'METRIC'}\n",
    "\n",
    "    categories_copy = entities.copy()\n",
    "\n",
    "    for category, subcategories in categories_copy.items():\n",
    "        # Convert the set of subcategories to a sorted list\n",
    "        sorted_subcategories = sorted(subcategories, key=str)\n",
    "        # If the category is in old_new_categories, replace it with the new category\n",
    "        if category in old_new_categories: \n",
    "            #This here is to replace the old names with new like desc -> METRIC\n",
    "            #Normally we d use it only for the METRIC, but left this way in case the principle will extend to other categories\n",
    "            #see old_new_categories above.\n",
    "\n",
    "            new_category = old_new_categories[category]\n",
    "            entities[new_category] = entities.pop(category)\n",
    "            # Replace the original set of subcategories with the modified list\n",
    "            sorted_subcategories.insert(0, '_ALL_'+new_category+'S_')\n",
    "            entities[new_category] = sorted_subcategories\n",
    "        else: #if we dont want to rename categories\n",
    "            sorted_subcategories.insert(0, '_ALL_'+category+'s_')\n",
    "            entities[category] = sorted_subcategories\n",
    "\n",
    "    #From METRIC remove whatever is not metric. \n",
    "    #Cos METRIC is originally a desc entity which can contain just anything:\n",
    "            \n",
    "    if 'METRIC' in entities:\n",
    "        valid_metrics = ['_ALL_METRICS_', 'STDs', 'PSDs', 'PtPsManual', 'PtPsAuto', 'ECGs', 'EOGs', 'Head', 'Muscle']\n",
    "        entities['METRIC'] = [x for x in entities['METRIC'] if x.lower() in [metric.lower() for metric in valid_metrics]]\n",
    "\n",
    "    return entities\n",
    "\n",
    "entities = modify_entity_name(entities)\n",
    "print('___MEGqc___: ', 'Entities after modification: ', entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = {\n",
    "    'subject': {'009'},\n",
    "    'session': {'1'},\n",
    "    'task': {'deduction'},\n",
    "    'run': {1},\n",
    "    'description': {'STDs', 'Muscle', 'ECGchannel', 'PSDwavesGrad', 'PtPsManual', 'Sensors', 'SimpleMetrics', 'RawInfo', 'PSDs', 'ReportStrings', 'PSDnoiseGrad', 'PSDwavesMag', 'ECGs', 'EOGchannel', 'PSDnoiseMag', 'EOGs'}\n",
    "}\n",
    "print('entities: ', entities)\n",
    "\n",
    "def modify_entity_name(entities):\n",
    "    # Create a copy of entities\n",
    "    categories = entities.copy()\n",
    "\n",
    "    # Rename 'description' to 'METRIC' and sort the values\n",
    "    categories = {\n",
    "        ('METRIC' if k == 'description' else k): sorted(v, key=str)\n",
    "        for k, v in categories.items()\n",
    "    }\n",
    "\n",
    "    #From METRIC remove whatever is not metric. \n",
    "    #Cos METRIC is originally a desc entity which can contain just anything:\n",
    "                \n",
    "    if 'METRIC' in categories:\n",
    "        valid_metrics = ['_ALL_METRICS_', 'STDs', 'PSDs', 'PtPsManual', 'PtPsAuto', 'ECGs', 'EOGs', 'Head', 'Muscle']\n",
    "        categories['METRIC'] = [x for x in categories['METRIC'] if x.lower() in [metric.lower() for metric in valid_metrics]]\n",
    "\n",
    "    #add '_ALL_' to the beginning of the list for each category:\n",
    "\n",
    "    for category, subcategories in categories.items():\n",
    "        categories[category] = ['_ALL_'+category+'s_'] + subcategories\n",
    "\n",
    "    return categories\n",
    "\n",
    "print('categories: ', categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import re\n",
    "\n",
    "# Load the CTF dataset\n",
    "data = '/Volumes/SSD_DATA/MEG_data/CTF/ds000246/sub-0001/meg/sub-0001_task-AEF_run-01_meg.ds'\n",
    "data = '/Volumes/SSD_DATA/MEG_data/CTF/ds000247/sub-0003/ses-01/meg/sub-0003_ses-01_task-rest_run-01_meg.ds'\n",
    "data = '/Volumes/SSD_DATA/MEG_data/CTF/ds002761/sub-311/meg/sub-311_task-loc_run-01_meg.ds'\n",
    "raw = mne.io.read_raw_ctf(data, preload=True)\n",
    "\n",
    "\n",
    "#data = '/Volumes/SSD_DATA/MEG_data/openneuro/ds003483/sub-009/ses-1/meg/sub-009_ses-1_task-deduction_run-1_meg.fif'\n",
    "#raw = mne.io.read_raw_fif(data, preload=True)\n",
    "\n",
    "print(raw.filenames)\n",
    "\n",
    "# Loop through each channel and print its type and unit\n",
    "# for nch, ch in enumerate(raw.info['chs']):\n",
    "#     ch_name = ch['ch_name']\n",
    "#     ch_type = mne.channel_type(raw.info, nch)  # Correct function to get channel type\n",
    "#     ch_unit = ch['unit']  # Get the unit code\n",
    "#     print(f\"Channel: {ch_name}, Type: {ch_type}, Unit: {ch_unit}\")\n",
    "\n",
    "picked_channels = mne.pick_types(raw.info, meg='mag')\n",
    "for ch in picked_channels:\n",
    "    ch_name = raw.info['ch_names'][ch]\n",
    "    print(f\"Channel: {ch_name}\")\n",
    "    ch_unit_code = raw.info['chs'][ch]['unit']\n",
    "    print(str(ch_unit_code))\n",
    "    ch_unit_str = str(ch_unit_code)\n",
    "    #find str after 'UNIT_' in ch_unit_code:\n",
    "    match = re.search(r'UNIT_(\\w)', ch_unit_str)\n",
    "    if match:\n",
    "        unit = match.group(1)\n",
    "    else:\n",
    "        unit = 'unknown'\n",
    "    print(f\"Unit: {unit}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw.set_channel_types({'1': 'grad', '2': 'mag', '3': 'mag'})\n",
    "\n",
    "ch = 31\n",
    "ch_name = raw.info['ch_names'][ch]\n",
    "print(f\"Channel: {ch_name}\")\n",
    "print(raw.info['chs'][ch]['kind'])\n",
    "print('ch type: ', mne.channel_type(raw.info, ch))\n",
    "ch_unit_code = raw.info['chs'][ch]['unit']\n",
    "print(str(ch_unit_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "raw2 = copy.deepcopy(raw)\n",
    "ch2 = 32\n",
    "raw2.set_channel_types({'MLC12-2910': 'grad'})\n",
    "\n",
    "ch_name = raw2.info['ch_names'][ch]\n",
    "print(f\"Channel: {ch_name}\")\n",
    "print(raw2.info['chs'][ch]['kind'])\n",
    "print('ch type: ', mne.channel_type(raw2.info, ch))\n",
    "ch_unit_code = raw2.info['chs'][ch]['unit']\n",
    "print(str(ch_unit_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "# Load MEG CTF data\n",
    "data = '/Volumes/SSD_DATA/MEG_data/CTF/ds000246/sub-0001/meg/sub-0001_task-AEF_run-01_meg.ds'\n",
    "raw = mne.io.read_raw_ctf(data, preload=True)\n",
    "\n",
    "# Get channel information\n",
    "info = raw.info\n",
    "\n",
    "# Check if a channel is mag or grad\n",
    "mags = mne.pick_types(info, meg='mag')\n",
    "grads = mne.pick_types(info, meg='grad')\n",
    "\n",
    "print(f'Magnetometer channels: {len(mags)}')\n",
    "print(f'Gradiometer channels: {len(grads)}')\n",
    "\n",
    "#print gradiometer channels with names and unit:\n",
    "mag_channels = [[info['ch_names'][ch], info['chs'][ch]['unit'], info['chs'][ch]['kind']] for ch in mags]\n",
    "grad_channels = [[info['ch_names'][ch], info['chs'][ch]['unit'], info['chs'][ch]['kind']] for ch in grads]\n",
    "print('MAGS', mag_channels)\n",
    "print('GRADS', grad_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mne.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['chs'][56]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['chs'][57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "info = raw.info\n",
    "\n",
    "# Step 2: Save the info object to a .fif file\n",
    "mne.io.write_info('info.fif', info)\n",
    "\n",
    "# Step 3: Load the info object from the file when needed\n",
    "info_loaded = mne.io.read_info('info.fif')\n",
    "\n",
    "# Step 4: Create an MNE Report\n",
    "report = mne.Report(title='MEG Report with Info')\n",
    "\n",
    "# Step 5: Convert info to a string format for HTML embedding\n",
    "info_html = info_loaded._repr_html_()\n",
    "\n",
    "\n",
    "# Step 6: Add the info to the report as an HTML block\n",
    "report.add_html(info_html, 'Info Object')\n",
    "\n",
    "# Step 7: Save the report to an HTML file\n",
    "report.save('meg_report_with_info.html', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Channel Kind and Units\n",
    "\n",
    "from mne.io.constants import FIFF\n",
    "import copy\n",
    "\n",
    "raw_new = copy.deepcopy(raw)\n",
    "\n",
    "# Get the info structure containing channel info\n",
    "info = raw_new.info\n",
    "\n",
    "# Function to convert all magnetometer channels to gradiometers\n",
    "for ch in info['chs']:\n",
    "    # If the channel is a magnetometer\n",
    "    print('ch[\"kind\"]', ch['kind'])\n",
    "    if ch['kind'] == FIFF.FIFFV_MEG_MAG:\n",
    "        # Change the channel type to gradiometer\n",
    "        ch['kind'] = FIFF.FIFFV_MEG_GRAD\n",
    "        # Change the unit to Tesla per meter (T/m)\n",
    "        ch['unit'] = FIFF.FIFF_UNIT_T_M\n",
    "\n",
    "# Optionally, check that the channels were updated correctly\n",
    "for ch in info['chs']:\n",
    "    if ch['kind'] == FIFF.FIFFV_MEG_GRAD:\n",
    "        print(f\"Channel {ch['ch_name']} is now a gradiometer with unit {mne.io.constants.FIFF.unit_dict[ch['unit']]}.\")\n",
    "\n",
    "# Save the modified data (if needed)\n",
    "raw_new.save('modified_meg.fif', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ancpbids issues:\n",
    "import ancpbids\n",
    "\n",
    "dataset_path = '/Volumes/SSD_DATA/MEG_QC_stuff/data/openneuro/ds003483' #ds000117'\n",
    "dataset = ancpbids.load_dataset(dataset_path)\n",
    "entities = dataset.query_entities(scope='derivatives')\n",
    "\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load CTF data\n",
    "data = '/Volumes/SSD_DATA/MEG_data/CTF/ds000246/sub-0001/meg/sub-0001_task-AEF_run-01_meg.ds'\n",
    "raw = mne.io.read_raw_ctf(data, preload=True)\n",
    "\n",
    "# Access the info structure\n",
    "info = raw.info\n",
    "\n",
    "# Print the channel names\n",
    "print(\"All channel names:\")\n",
    "print(info['ch_names'])\n",
    "\n",
    "# Get the types of channels\n",
    "picks_grad = data.copy().pick('grad').ch_names if 'grad' in data else []\n",
    "\n",
    "# Check the number of gradiometers\n",
    "n_gradiometers = sum([1 for t in picks_grad if t == 'grad'])\n",
    "print(f\"Number of gradiometers: {n_gradiometers}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_in_path = \"sub-01_ses-meg_task-facerecognition_run-01_proc-sss_meg\"\n",
    "file_name_in_obj = \"sub-01_ses-meg_task-facerecognition_run-01_meg\"\n",
    "\n",
    "if file_name_in_obj not in file_name_in_path:\n",
    "    raise ValueError('Different names in list_of_files and entities_per_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ancpbids\n",
    "\n",
    "dataset_path = '/Volumes/SSD_DATA/MEG_data/CTF/ds000246'\n",
    "sid = '0001'\n",
    "dataset = ancpbids.load_dataset(dataset_path)\n",
    "\n",
    "artifacts = dataset.query(suffix=\"meg\", return_type=\"object\", subj=sid)\n",
    "# convert to folders of found files\n",
    "folders = map(lambda a: a.get_parent().get_absolute_path(), artifacts)\n",
    "# remove duplicates\n",
    "folders = set(folders)\n",
    "# convert to liust before filtering\n",
    "folders = list(folders)\n",
    "\n",
    "# filter for folders which end with \".ds\" (including os specific path separator)\n",
    "# folders = list(filter(lambda f: f.endswith(f\"{os.sep}.ds\"), folders))\n",
    "\n",
    "# Filter for folders which end with \".ds\"\n",
    "filtered_folders = [f for f in folders if f.endswith('.ds')]\n",
    "\n",
    "print(filtered_folders)\n",
    "\n",
    "entities_per_file = dataset.query(sub=sid, suffix='meg', scope='raw')\n",
    "print(entities_per_file)\n",
    "\n",
    "entities_per_file_res4 = dataset.query(sub=sid, suffix='meg', extension='.res4')\n",
    "\n",
    "print(entities_per_file_res4)\n",
    "\n",
    "print(entities_per_file_res4[0].name)\n",
    "\n",
    "#In entities_per_file filer out only thouse who have unique name, disregard the extension in this name:\n",
    "#Return entities_per_file_filtered as list of objects\n",
    "\n",
    "entities_per_file_filtered = []\n",
    "\n",
    "for entity in entities_per_file:\n",
    "    name = entity.name.split('.')[0]\n",
    "    if name not in entities_per_file_filtered:\n",
    "        entities_per_file_filtered.append(entity)\n",
    "\n",
    "print(entities_per_file_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ancpbids\n",
    "import os\n",
    "\n",
    "dataset_path = '/Users/jenya/Local Storage/Job Uni Rieger lab/data/ds83'\n",
    "sid = '009'\n",
    "dataset = ancpbids.load_dataset(dataset_path)\n",
    "\n",
    "list_of_files = sorted(list(dataset.query(suffix='meg', extension='.fif', return_type='filename', subj=sid)))\n",
    "entities_per_file = dataset.query(sub=sid, suffix='meg', extension='.fif')\n",
    "# sort list_of_sub_jsons by name key to get same order as list_of_files\n",
    "entities_per_file = sorted(entities_per_file, key=lambda k: k['name'])\n",
    "\n",
    "#check we got same number of jsons and files:\n",
    "if len(list_of_files) != len(entities_per_file):\n",
    "    raise ValueError('Different number of files and jsons')\n",
    "\n",
    "print(list_of_files)\n",
    "print(entities_per_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts = dataset.query(suffix=\"meg\", return_type=\"object\", subj=sid)\n",
    "\n",
    "print(artifacts)\n",
    "# convert to folders of found files\n",
    "folders = map(lambda a: a.get_parent().get_absolute_path(), artifacts)\n",
    "# remove duplicates\n",
    "folders = set(folders)\n",
    "# convert to liust before filtering\n",
    "folders = list(folders)\n",
    "\n",
    "print(folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = dataset.get_schema()\n",
    "derivative = dataset.create_derivative(name=\"Meg_QC\")\n",
    "derivative.dataset_description.GeneratedBy.Name = \"MEG QC Pipeline\"\n",
    "subject_folder = derivative.create_folder(type_=schema.Subject, name='sub-'+sid)\n",
    "calculation_folder = subject_folder.create_folder(name='calculation')\n",
    "\n",
    "meg_artifact = calculation_folder.create_artifact(raw=entities_per_file_res4[0])\n",
    "#meg_artifact = calculation_folder.create_artifact(raw=entities_per_file[0])\n",
    "\n",
    "print(meg_artifact.get_absolute_path())\n",
    "\n",
    "meg_artifact.add_entity('desc', 'TEST') #file name\n",
    "meg_artifact.suffix = 'meg'\n",
    "meg_artifact.extension = '.txt'\n",
    "meg_artifact.content = 'dummy text'\n",
    "\n",
    "ancpbids.write_derivative(dataset, derivative) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "\n",
    "file_path = '/Volumes/SSD_DATA/MEG_data/CTF/ds000246/sub-0001/meg/sub-0001_task-AEF_run-01_meg.ds'\n",
    "raw = mne.io.read_raw_ctf(file_path, preload=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ancpbids\n",
    "\n",
    "metric = 'test'\n",
    "\n",
    "dataset = ancpbids.load_dataset('/Volumes/SSD_DATA/camcan')\n",
    "entities = dataset.query_entities()\n",
    "\n",
    "#for sub in chosen_entities['subject']:\n",
    "sub = 'CC220697'\n",
    "\n",
    "entities_per_file = dataset.query(sub=sub, suffix='meg', extension='.fif')\n",
    "print('___MEGqc___: list_of_sub_jsons', entities_per_file)\n",
    "sub_json = entities_per_file[0]\n",
    "print('___MEGqc___: sub_json', sub_json)\n",
    "\n",
    "\n",
    "#Want: get entities of sub jason: sub, ses, run, task.\n",
    "# find corresponding tsv file with same entities and use it to create report.\n",
    "# save report with the same entities\n",
    "\n",
    "schema = dataset.get_schema()\n",
    "\n",
    "derivative = dataset.create_derivative(name=\"Meg_QC\")\n",
    "derivative.dataset_description.GeneratedBy.Name = \"MEG QC Pipeline\"\n",
    "subject_folder = derivative.create_folder(type_=schema.Subject, name='sub-'+sub)\n",
    "#strings = dataset.query(suffix='meg', extension='.json', return_type='filename', subj=sub, ses = chosen_entities['session'], task = chosen_entities['task'], run = chosen_entities['run'], desc = 'ReportStrings', scope='derivatives')\n",
    "strings = dataset.query(suffix='meg', extension='.json', return_type='filename', subj=sub, desc = 'ReportStrings', scope='derivatives')\n",
    "print(strings)\n",
    "\n",
    "# Now prepare the derivative to be written:\n",
    "meg_artifact = subject_folder.create_artifact(raw=sub_json)\n",
    "meg_artifact.add_entity('desc', metric) #file name\n",
    "meg_artifact.suffix = 'meg'\n",
    "meg_artifact.extension = '.html'\n",
    "\n",
    "\n",
    "#0. Get entities of each of tsvs_to_plot, save them into a dictionary with the tsv path as key and entities as values.\n",
    "#1. Get all sub jsons and their entities, save into a dictionary with the json path as key and entities as values.\n",
    "#2. Find the tsvs that have the same entities as the jsons.\n",
    "#3. Make a dict: key = json, value = tsvs.\n",
    "\n",
    "\n",
    "#OR:\n",
    "#get all tsv files for 1 metric\n",
    "#match what sub json (before '_meg') were they created for by just matching string up until '_desc'\n",
    "\n",
    "\n",
    "strings = dataset.query(suffix='meg', extension='.json', return_type='filename', subj=sub, desc = 'ReportStrings', scope='derivatives')\n",
    "\n",
    "\n",
    "2.\n",
    "\n",
    "\n",
    "\n",
    "#2. Loop: get entities of 1 json(raw).\n",
    "\n",
    "    #3. Match the entities of the json with the entities of the tsvs.\n",
    "\n",
    "    #4. If match: create a derivative from these tsvs.\n",
    "\n",
    "    #5. Save this derivative with the entities of the json.\n",
    "\n",
    "#6. Repeat for all jsons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sub_json = {'name': 'sub-CC220697_ses-passive_task-passive_desc-ECGs', 'extension': '.fif', 'suffix': 'meg'}\n",
    "\n",
    "# match = re.search(r'sub-(\\w+)(_ses-(\\w+))?(_task-(\\w+))?(_run-(\\w+))?', sub_json['name'])\n",
    "# match = re.search(r'sub-(\\w+)(?:_ses-(\\w+))?(?:_task-(\\w+))?(?:_run-(\\w+))?', sub_json['name'])\n",
    "# match = re.search(r'sub-(\\w+)_ses-(\\w+)_task-(\\w+)_run-(\\w+)', sub_json['name'])\n",
    "match = re.search(r'sub-(\\w+)_ses-(\\w+)_task-(\\w+)_desc-(\\w+)', sub_json['name'])\n",
    "\n",
    "print(match.group(1))\n",
    "print(match.group(2))\n",
    "print(match.group(3))\n",
    "print(match.group(4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_entitities = []\n",
    "for entity in ['sub', 'ses', 'task', 'run']:\n",
    "    if entity in sub_json['name']:\n",
    "        json_entitities.append(entity)\n",
    "\n",
    "#now combine json_entitities in a string and find values for them in the json file name:\n",
    "#then compare them with the values of the tsv file name:\n",
    "\n",
    "match_str = ''\n",
    "for entity in json_entitities:\n",
    "    match_str += entity + '-(\\w+)_'\n",
    "#cut the underscore at the end:\n",
    "match_str = match_str[:-1]\n",
    "#shouldd look like:\n",
    "#match_str = 'sub-(\\w+)_ses-(\\w+)_task-(\\w+)_run-(\\w+)'\n",
    "\n",
    "\n",
    "match = re.search(match_str, sub_json['name'])\n",
    "\n",
    "#print all existing groups:\n",
    "for i in range(1, len(match.groups())+1):\n",
    "    print(match.group(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def normalize_wave(wave):\n",
    "    mean = np.mean(wave)\n",
    "    std = np.std(wave)\n",
    "    return (wave - mean) / std\n",
    "\n",
    "def compute_shape_correlation(wave1, wave2):\n",
    "    # Pearson correlation yielding a value between -1 and 1\n",
    "    correlation, _ = pearsonr(wave1, wave2)\n",
    "    return correlation\n",
    "\n",
    "def compute_mean_amplitude(wave):\n",
    "    # Calculate the mean amplitude of the wave\n",
    "    return np.mean(np.abs(wave))\n",
    "\n",
    "def normalize_amplitudes(amplitudes):\n",
    "    # Normalize amplitudes to a range of 0 to 1\n",
    "    min_amp = np.min(amplitudes)\n",
    "    max_amp = np.max(amplitudes)\n",
    "    return (amplitudes - min_amp) / (max_amp - min_amp)\n",
    "\n",
    "def compute_combined_score(shape_similarity, normalized_amplitude, alpha=0.5, beta=0.5):\n",
    "    return alpha * shape_similarity + beta * normalized_amplitude\n",
    "\n",
    "# Example usage\n",
    "reference_wave = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "waves_to_compare = [\n",
    "    np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20]),  # Similar shape, higher amplitude\n",
    "    np.array([1, 2, 3, 3, 5, 6, 7, 7, 9, 10]),      # Similar shape, varied amplitude\n",
    "    np.array([10, 9, 8, 7, 6, 5, 4, 3, 2, 1]),      # Different shape, similar amplitude\n",
    "    np.array([0, 0, 1, 0, 0, 0, 0, 0, 5, 0])        # Flat wave, different shape and amplitude\n",
    "]\n",
    "\n",
    "# Normalize the reference wave\n",
    "normalized_ref_wave = normalize_wave(reference_wave)\n",
    "\n",
    "print('normalized_ref_wave:',normalized_ref_wave)\n",
    "\n",
    "\n",
    "# Calculate shape similarity and amplitudes\n",
    "shape_similarities = []\n",
    "amplitudes = []\n",
    "\n",
    "for wave in waves_to_compare:\n",
    "    # Normalize the target wave\n",
    "    normalized_wave = normalize_wave(wave)\n",
    "    print('normalized_wave:',normalized_wave)\n",
    "    \n",
    "    # Compute shape similarity\n",
    "    shape_similarity = compute_shape_correlation(normalized_ref_wave, normalized_wave)\n",
    "    shape_similarities.append(shape_similarity)\n",
    "    \n",
    "    # Compute mean amplitude (we're using the original waves for amplitude)\n",
    "    amplitude = compute_mean_amplitude(wave)\n",
    "    amplitudes.append(amplitude)\n",
    "\n",
    "# Normalize the amplitudes to a [0, 1] range\n",
    "normalized_amplitudes = normalize_amplitudes(np.array(amplitudes))\n",
    "\n",
    "# Compute combined scores\n",
    "combined_scores = []\n",
    "\n",
    "for shape_similarity, normalized_amplitude in zip(shape_similarities, normalized_amplitudes):\n",
    "    score = compute_combined_score(shape_similarity, normalized_amplitude, alpha=0.5, beta=0.5)\n",
    "    combined_scores.append(score)\n",
    "\n",
    "# Rank the waves based on the combined score\n",
    "ranked_indices = np.argsort(combined_scores)[::-1]  # Sort in descending order\n",
    "\n",
    "# Display the results\n",
    "for index in ranked_indices:\n",
    "    print(f\"Shape similarity: {shape_similarities[index]:.2f}, Normalized amplitude: {normalized_amplitudes[index]:.2f}, Combined score: {combined_scores[index]:.2f}\")\n",
    "\n",
    "\n",
    "#Plot with plotly the waves and the refernce wave:\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the reference wave\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(reference_wave)), y=reference_wave, mode='lines', name='Reference wave'))\n",
    "\n",
    "# Add the waves to compare\n",
    "for i, wave in enumerate(waves_to_compare):\n",
    "    fig.add_trace(go.Scatter(x=np.arange(len(wave)), y=wave, mode='lines', name=f'Wave {i + 1}'))\n",
    "\n",
    "fig.update_layout(title='Comparison of Waves',\n",
    "                    xaxis_title='Time',\n",
    "                    yaxis_title='Amplitude')\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the ECG correlation algorythm here:\n",
    "\n",
    "import numpy as np\n",
    "#read tsv file into a data frame:\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/jenya/Local Storage/Job Uni Rieger lab/data/ds83/derivatives/Meg_QC/sub-009/sub-009_ses-1_task-deduction_run-1_desc-ECGs_meg.tsv', sep='\\t')\n",
    "\n",
    "metric = 'ECG'\n",
    "ecg_eog_scores = ['ecg_corr_coeff', 'ecg_amplitude_ratio', 'ecg_similarity_score']\n",
    "add_scores = all(column_name in df.columns and not df[column_name].empty and df[column_name].notnull().any() for column_name in ecg_eog_scores)\n",
    "print(add_scores)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(row['Name'])\n",
    "    print(row[ecg_eog_scores[0]])\n",
    "    print(row[ecg_eog_scores[1]])\n",
    "    print(row[ecg_eog_scores[2]])\n",
    "    customdata=np.array([row[ecg_eog_scores[0]], row[ecg_eog_scores[1]], row[ecg_eog_scores[2]]])\n",
    "    print(customdata)\n",
    "\n",
    "df_sorted = df.reindex(df['ecg_corr_coeff'].abs().sort_values(ascending=False).index)\n",
    "\n",
    "metric='ecg'\n",
    "#print out the name of the channels and the corr_coeff of this channel in the sorted list:\n",
    "#for index, row in df_sorted.iterrows():\n",
    "#    print('______', row['Name'], row[metric.lower()+'_corr_coeff'])\n",
    "\n",
    "\n",
    "df_ref = pd.read_csv('/Users/jenya/Local Storage/Job Uni Rieger lab/data/ds83/derivatives/Meg_QC/sub-009/sub-009_ses-1_task-deduction_run-1_desc-ECGchannel_meg.tsv', sep='\\t')\n",
    "\n",
    "#make several plots: each contains 1 channel and the reference channel mean_rwave_shifted\n",
    "\n",
    "#refernce: mean_rwave_shifted column in df_ref:\n",
    "ch_data_ref = df_ref['mean_rwave_shifted']\n",
    "#remove nans:\n",
    "ch_data_ref = ch_data_ref.dropna()\n",
    "#Scale the data from -1 to 1:\n",
    "ch_data_ref = (ch_data_ref - ch_data_ref.min())/(ch_data_ref.max()-ch_data_ref.min())*2-1\n",
    "\n",
    "#time:\n",
    "t = df_ref['mean_rwave_time']\n",
    "\n",
    "#Channels:\n",
    "col_prefix = 'smoothed_mean_ecg_sec_'\n",
    "\n",
    "chs_vals = {}\n",
    "chs_vals_scaled = {}\n",
    "ch_to_plot = ['MEG1531', 'MEG1131', 'MEG1541', 'MEG0111', 'MEG1031', 'MEG0621']\n",
    "for ch in ch_to_plot:\n",
    "    ch_data = []\n",
    "    for index, row in df_sorted.iterrows():\n",
    "        if row['Name'] == ch: #plot only mag/grad\n",
    "            for col in df.columns:\n",
    "                if col_prefix in col:\n",
    "                    #ch_data = row[col] #or maybe \n",
    "                    ch_data.append(row[col])\n",
    "\n",
    "    #Scale ch_data list from -1 to 1:\n",
    "    min_val = min(ch_data)\n",
    "    max_val = max(ch_data)\n",
    "    \n",
    "    ch_data_scaled = [(2*(x - min_val) / (max_val - min_val)) - 1 for x in ch_data]\n",
    "\n",
    "    chs_vals[ch] = ch_data\n",
    "    chs_vals_scaled[ch] = ch_data_scaled\n",
    "\n",
    "\n",
    "# for each channel calculate pearson correlation p value with the reference channel:\n",
    "corr_vals = {}\n",
    "corr_vals_scaled = {}\n",
    "from scipy.stats import pearsonr\n",
    "for ch in ch_to_plot:\n",
    "    corr, p_val = pearsonr(ch_data_ref, chs_vals[ch])\n",
    "    corr_scaled, p_val_scaled = pearsonr(ch_data_ref, chs_vals_scaled[ch])\n",
    "    #save to corr_vals:\n",
    "    #round corr to 3 decimal places:\n",
    "    corr = round(corr, 3)\n",
    "    corr_vals[ch] = corr\n",
    "    corr_scaled = round(corr_scaled, 3)\n",
    "    corr_vals_scaled[ch] = corr_scaled\n",
    "\n",
    "print('corr_vals:', corr_vals)\n",
    "print('corr_vals_scaled', corr_vals_scaled)\n",
    "\n",
    "\n",
    "# make plots with plotly:\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(rows=3, cols=2, subplot_titles=('MEG1531 (most)'+', corr: '+str(corr_vals['MEG1531']), 'MEG1131 (most)'+', corr: '+str(corr_vals['MEG1131']), 'MEG1541 (mid)'+', corr: '+str(corr_vals['MEG1541']), 'MEG0111 (mid)'+', corr: '+str(corr_vals['MEG0111']), 'MEG1031 (least)'+', corr: '+str(corr_vals['MEG1031']), 'MEG0621 (least)'+', corr: '+str(corr_vals['MEG0621'])))\n",
    "\n",
    "for i, ch in enumerate(ch_to_plot):\n",
    "    fig.add_trace(go.Scatter(x=t, y=ch_data_ref, mode='lines', name='ref', line=dict(color='black')), row=i//2+1, col=i%2+1)\n",
    "    fig.add_trace(go.Scatter (x=t, y=chs_vals_scaled[ch], mode='lines', name=ch), row=i//2+1, col=i%2+1)\n",
    "    #for each channel add their corr value into the subplot title:\n",
    "\n",
    "\n",
    "fig.update_layout(title_text='ECG channels')\n",
    "#make subplot titles smaller:\n",
    "# Update the font size of the subplot titles\n",
    "for i in fig['layout']['annotations']:\n",
    "    i['font'] = dict(size=10)\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n",
    "#Now plot all 6 channels in 1 plot with scaling:\n",
    "fig = go.Figure()\n",
    "for ch in ch_to_plot:\n",
    "    fig.add_trace(go.Scatter(x=t, y=chs_vals_scaled[ch], mode='lines', name=ch+': '+str(corr_vals_scaled[ch])))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "#and same without scaling:\n",
    "fig = go.Figure()\n",
    "for ch in ch_to_plot:\n",
    "    fig.add_trace(go.Scatter(x=t, y=chs_vals[ch], mode='lines', name=ch+': '+str(corr_vals[ch])))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "#Look especially at 1131: it has very small amplitude, but the correlation is very high. \n",
    "#Because pearson cares only about the shape, not amplitude. so it basically sees the scaled version of each channel.\n",
    "#and in the scaled version this channel is very well correlated.\n",
    "#Note: disregard the corr sign, we care about abs corr value, cos signe is just defined by \n",
    "#the placement of the electrode on the head against the magnetic field\n",
    "\n",
    "#So again, we should think of how to include the amplitude into the correlation calculation.\n",
    "#And maybe define some corre value at which the channels with be separated into most/middle/least affected,\n",
    "#not just equally devide into 3 groups.\n",
    "# For example here 4 channels have very similar strong correlation(0.7-0.6), but they r divided into 2 most and lest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p;laying with hovertemplate here:\n",
    "\n",
    "\n",
    "import random\n",
    "import numpy as np  \n",
    "\n",
    "def plot_df_of_channels_data_as_lines_by_lobe_csv(f_path: str, metric: str, x_values, m_or_g, df=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Plots data from a data frame as lines, each lobe has own color.\n",
    "    Data is taken from previously saved tsv file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    f_path : str\n",
    "        Path to the csv file with the data to plot.\n",
    "    metric : str\n",
    "        The metric of the data to plot: 'psd', 'ecg', 'eog', 'smoothed_ecg', 'smoothed_eog'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    fig : plotly.graph_objects.Figure\n",
    "        Plotly figure.\n",
    "\n",
    "    \"\"\"\n",
    "    if f_path is not None:\n",
    "        df = pd.read_csv(f_path, sep='\\t') #TODO: maybe remove reading csv and pass directly the df here?\n",
    "    else:\n",
    "        df = df\n",
    "\n",
    "\n",
    "    fig = go.Figure()\n",
    "    traces_lobes=[]\n",
    "    traces_chs=[]\n",
    "\n",
    "    add_scores = False #in most cases except ecg/eog we dont add scores to the plot\n",
    "    if metric.lower() == 'psd':\n",
    "        col_prefix = 'PSD_Hz_'\n",
    "    elif metric.lower() == 'ecg':\n",
    "        col_prefix = 'mean_ecg_sec_'\n",
    "    elif metric.lower() == 'eog':\n",
    "        col_prefix = 'mean_eog_sec_'\n",
    "    elif metric.lower() == 'smoothed_ecg' or metric.lower() == 'ecg_smoothed':\n",
    "        col_prefix = 'smoothed_mean_ecg_sec_'\n",
    "        #Need to check if all 3 columns exist in df, are not empty and are not none - if so, add scores to hovertemplate:\n",
    "        ecg_eog_scores = ['ecg_corr_coeff', 'ecg_amplitude_ratio', 'ecg_similarity_score']\n",
    "        add_scores = all(column_name in df.columns and not df[column_name].empty and df[column_name].notnull().any() for column_name in ecg_eog_scores)\n",
    "    elif metric.lower() == 'smoothed_eog' or metric.lower() == 'eog_smoothed':\n",
    "        col_prefix = 'smoothed_mean_eog_sec_'\n",
    "        #Need to check if all 3 columns exist in df, are not empty and are not none - if so, add scores to hovertemplate:\n",
    "        ecg_eog_scores = ['eog_corr_coeff', 'eog_amplitude_ratio', 'eog_similarity_score']\n",
    "        add_scores = all(column_name in df.columns and not df[column_name].empty and df[column_name].notnull().any() for column_name in ecg_eog_scores)\n",
    "    else:\n",
    "        print('No proper column in df! Check the metric!')\n",
    "\n",
    "   \n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        if row['Type'] == m_or_g: #plot only mag/grad\n",
    "            ch_data = []\n",
    "            for col in df.columns:\n",
    "                if col_prefix in col:\n",
    "\n",
    "                    #ch_data = row[col] #or maybe \n",
    "                    ch_data.append(row[col])\n",
    "\n",
    "                    # normally color must be same for all channels in lobe, so we could assign it before the loop as the color of the first channel,\n",
    "                    # but here it is done explicitly for every channel so that if there is any color error in chs_by_lobe, it will be visible\n",
    "            \n",
    "            color = row['Lobe Color']\n",
    "\n",
    "            #traces_chs += [go.Scatter(x=x_values, y=ch_data, line=dict(color=color), name=row['Name'] , legendgroup=row['Lobe'] , legendgrouptitle=dict(text=row['Lobe'].upper(), font=dict(color=color)))]\n",
    "\n",
    "            if add_scores:\n",
    "                \n",
    "                traces_chs += [go.Scatter(\n",
    "                    x=x_values, \n",
    "                    y=ch_data, \n",
    "                    line=dict(color=color), \n",
    "                    name=row['Name'],\n",
    "                    legendgroup=row['Lobe'],\n",
    "                    legendgrouptitle=dict(text=row['Lobe'].upper(), font=dict(color=color)),\n",
    "                    hovertemplate = (\n",
    "                    '<b>'+row['Name']+'</b><br>' +\n",
    "                    'time: %{x}, magnitude: %{y}<br>' +\n",
    "                    '<i>corr_coeff: </i>'+'{:.2f}'.format(row[ecg_eog_scores[0]])+'<br>' +\n",
    "                    '<i>amplitude_ratio: </i>'+'{:.2f}'.format(row[ecg_eog_scores[1]])+'<br>' +\n",
    "                    '<i>similarity_score: </i>'+'{:.2f}'.format(row[ecg_eog_scores[2]])+'<br>'\n",
    "                ))]\n",
    "            else:\n",
    "                print('_________NO SCORES TO ADD TO HOVER TEMPLATE_________')\n",
    "                traces_chs += [go.Scatter(\n",
    "                    x=x_values, \n",
    "                    y=ch_data, \n",
    "                    line=dict(color=color), \n",
    "                    name=row['Name'],\n",
    "                    legendgroup=row['Lobe'],\n",
    "                    legendgrouptitle=dict(text=row['Lobe'].upper(), font=dict(color=color))\n",
    "                )]\n",
    "            #legendgrouptitle is group tile on the plot. legendgroup is not visible on the plot - it s used for sorting the legend items in update_layout() below.\n",
    "\n",
    "                # hovertemplate=\n",
    "                #     '<b>%{y}</b><br>' +\n",
    "                #     row['Name']+'<br>' +\n",
    "                #     '<i>corr_coeff: </i>%{customdata[0]}<br>' +\n",
    "                #     '<i>amplitude_ratio: </i>%{customdata[1]}<br>' +\n",
    "                #     '<i>similarity_score: </i>%{customdata[2]}<br>',\n",
    "                # customdata=np.array([row[ecg_eog_scores[0]], row[ecg_eog_scores[1]], row[ecg_eog_scores[2]]]),\n",
    "                \n",
    "    # sort traces in random order:\n",
    "    # When you plot traves right away in the order of the lobes, all the traces of one color lay on top of each other and yu can't see them all.\n",
    "    # This is why they are not plotted in the loop. So we sort them in random order, so that traces of different colors are mixed.\n",
    "    traces = traces_lobes + sorted(traces_chs, key=lambda x: random.random())\n",
    "\n",
    "    downsampling_factor = 1  # replace with your desired downsampling factor\n",
    "    # Create a new list for the downsampled traces\n",
    "    traces_downsampled = []\n",
    "\n",
    "    # Go through each trace\n",
    "    for trace in traces:\n",
    "        # Downsample the x and y values of the trace\n",
    "        x_downsampled = trace['x'][::downsampling_factor]\n",
    "        y_downsampled = trace['y'][::downsampling_factor]\n",
    "\n",
    "        # Create a new trace with the downsampled values\n",
    "        trace_downsampled = go.Scatter(x=x_downsampled, y=y_downsampled, line=trace['line'], name=trace['name'], legendgroup=trace['legendgroup'], legendgrouptitle=trace['legendgrouptitle'])\n",
    "\n",
    "        # Add the downsampled trace to the list\n",
    "        traces_downsampled.append(trace_downsampled)\n",
    "\n",
    "\n",
    "    # Now first add these traces to the figure and only after that update the layout to make sure that the legend is grouped by lobe.\n",
    "    fig = go.Figure(data=traces)\n",
    "\n",
    "    fig.update_layout(legend_traceorder='grouped', legend_tracegroupgap=12, legend_groupclick='toggleitem')\n",
    "    #You can make it so when you click on lobe title or any channel in lobe you activate/hide all related channels if u set legend_groupclick='togglegroup'.\n",
    "    #But then you cant see individual channels, it turn on/off the whole group. There is no option to tun group off by clicking on group title. Grup title and group items behave the same.\n",
    "\n",
    "    #to see the legend: there is really nothing to sort here. The legend is sorted by default by the order of the traces in the figure. The onl way is to group the traces by lobe.\n",
    "    #print(fig['layout'])\n",
    "\n",
    "    #https://plotly.com/python/reference/?_ga=2.140286640.2070772584.1683497503-1784993506.1683497503#layout-legend-traceorder\n",
    "    \n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "metric = 'ecg_smoothed'\n",
    "f_path = '/Users/jenya/Local Storage/Job Uni Rieger lab/data/ds83/derivatives/Meg_QC/sub-009/sub-009_ses-1_task-deduction_run-1_desc-ECGs_meg.tsv'\n",
    "f_path2 = '/Users/jenya/Local Storage/Job Uni Rieger lab/data/ds83/derivatives/Meg_QC/sub-009/sub-009_ses-1_task-deduction_run-1_desc-ECGchannel_meg.tsv'\n",
    "m_or_g = 'mag'\n",
    "\n",
    "df = pd.read_csv(f_path, sep='\\t')\n",
    "df2 = pd.read_csv(f_path2, sep='\\t')\n",
    "\n",
    "x_values = df2['mean_rwave_time']\n",
    "fig = plot_df_of_channels_data_as_lines_by_lobe_csv(f_path, metric, x_values, m_or_g, df)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New idea: combine amplitude and correlation:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#Calculate amplitude ratio for each channel:\n",
    "\n",
    "# Function to calculate the Root Mean Square (RMS) amplitude\n",
    "def rms_amplitude(wave):\n",
    "    return np.sqrt(np.mean(np.square(wave)))\n",
    "\n",
    "# RMS amplitude of all comparison waves excluding the reference_wave\n",
    "rms_all_comp_waves = np.mean([rms_amplitude(wave) for wave in chs_vals.values()])\n",
    "\n",
    "amplitude_ratios = {}\n",
    "for ch in ch_to_plot:\n",
    "    #amplitude_ratio = rms_amplitude(chs_vals[ch]) / rms_amplitude(ch_data_ref)\n",
    "    amplitude_ratio = rms_amplitude(chs_vals[ch]) / rms_all_comp_waves  \n",
    "\n",
    "    amplitude_ratios[ch] = amplitude_ratio\n",
    "\n",
    "print('amplitude_ratios:', amplitude_ratios)\n",
    "print('corr_vals_scaled:', corr_vals_scaled)    \n",
    "\n",
    "\n",
    "# Combine the two metrics like: similarity_score = correlation * amplitude_ratio\n",
    "\n",
    "similarity_scores = {}\n",
    "for ch in ch_to_plot:\n",
    "    similarity_score = abs(corr_vals_scaled[ch]) * abs(amplitude_ratios[ch])\n",
    "    similarity_scores[ch] = similarity_score\n",
    "\n",
    "print('similarity_scores:', similarity_scores)\n",
    "\n",
    "#Now plot all 6 channels in 1 plot with similarity scores:\n",
    "fig = go.Figure()\n",
    "for ch in ch_to_plot:\n",
    "    fig.add_trace(go.Scatter(x=t, y=chs_vals[ch], mode='lines', name=ch+': '+str(similarity_scores[ch])))\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#ONE MORE idea for later: \n",
    "# scale all channels not just from-1 to 1 but rescale proportional to max amplitude of all channels:\n",
    "\n",
    "#Calculate max amplitude of all channels:\n",
    "max_amplitude = max([max(chs_vals[ch]) for ch in ch_to_plot])\n",
    "min_amplitude = min([min(chs_vals[ch]) for ch in ch_to_plot])\n",
    "\n",
    "#Scale all channels:\n",
    "chs_vals_scaled_max = {}\n",
    "for ch in ch_to_plot:\n",
    "    ch_data_scaled_max = [(2*(x - min_amplitude) / (max_amplitude - min_amplitude)) - 1 for x in chs_vals[ch]]\n",
    "    chs_vals_scaled_max[ch] = ch_data_scaled_max\n",
    "\n",
    "#Now plot all 6 channels in 1 plot with scaling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Empty list\n",
    "empty_list = []\n",
    "print(len(empty_list))  # Outputs: 0\n",
    "\n",
    "# Empty numpy array\n",
    "empty_array = np.array([])\n",
    "print(len(empty_array))  # Outputs: 0\n",
    "\n",
    "empty_list = empty_list if len(empty_list) else print('Empty list is empty')\n",
    "empty_array = empty_array if len(empty_array) else print('Empty numpy array is empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/openneuro/ds003483/sub-022/ses-1/meg/sub-022_ses-1_task-deduction_run-1_meg.fif', on_split_missing='ignore')\n",
    "\n",
    "#print(raw.info)\n",
    "print(type(raw.info))\n",
    "\n",
    "#How to extract raw.info from data file, save in derivs and later embed it into the mne report?\n",
    "\n",
    "rep = mne.Report(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from meg_qc.source.universal_plots import make_3d_sensors_trace, keep_unique_locs, switch_names_on_off, QC_derivative\n",
    "from meg_qc.source.initial_meg_qc import MEG_channels\n",
    "\n",
    "\n",
    "def plot_sensors_3d_csv(sensors_csv_path: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Plots the 3D locations of the sensors in the raw file. Plot both mags and grads (if both present) in 1 figure. \n",
    "    Can turn mags/grads visialisation on and off.\n",
    "    Separete channels into brain areas by color coding.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chs_by_lobe : dict\n",
    "        A dictionary of channels by ch type and lobe.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    qc_derivative : list\n",
    "        A list of QC_derivative objects containing the plotly figures with the sensor locations.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(sensors_csv_path, sep='\\t')\n",
    "\n",
    "\n",
    "    #to not rewrite the whole func, just turn the df back into dic of MEG_channels:\n",
    "\n",
    "    unique_lobes = df['Lobe'].unique().tolist()\n",
    "\n",
    "    lobes_dict={}\n",
    "    for lobe in unique_lobes:\n",
    "        lobes_dict[lobe] = []\n",
    "        for index, row in df.iterrows():\n",
    "            if row['Lobe'] == lobe:\n",
    "                locs = [row[col] for col in df.columns if 'Sensor_location' in col]\n",
    "                lobes_dict[lobe].append(MEG_channels(name = row['Name'], type = row['Type'], lobe = row['Lobe'], lobe_color = row['Lobe Color'], loc = locs))\n",
    "\n",
    "    print(lobes_dict)\n",
    "\n",
    "    traces = []\n",
    "\n",
    "    if len(lobes_dict)>1: #if there are lobes - we use color coding: one color pear each lobe\n",
    "        for lobe in lobes_dict:\n",
    "            ch_locs, ch_names, ch_color, ch_lobe = keep_unique_locs(lobes_dict[lobe])\n",
    "            traces.append(make_3d_sensors_trace(ch_locs, ch_names, ch_color[0], 10, ch_lobe[0], 'circle', 'top left'))\n",
    "            #here color and lobe must be identical for all channels in 1 trace, thi is why we take the first element of the list\n",
    "            # TEXT SIZE set to 10. This works for the \"Always show names\" option but not for \"Show names on hover\" option\n",
    "\n",
    "    else: #if there are no lobes - we use random colors previously assigned to channels, channel names will be used instead of lobe names in make_3d_trace function\n",
    "        ch_locs, ch_names, ch_color, ch_lobe = keep_unique_locs(lobes_dict[lobe])\n",
    "        for i, _ in enumerate(ch_locs):\n",
    "            traces.append(make_3d_sensors_trace([ch_locs[i]], ch_names[i], ch_color[i], 10, ch_names[i], 'circle', 'top left'))\n",
    "\n",
    "\n",
    "    fig = go.Figure(data=traces)\n",
    "\n",
    "    fig.update_layout(\n",
    "        width=900, height=900,\n",
    "        title={\n",
    "        'text': 'Sensors positions',\n",
    "        'y':0.85,\n",
    "        'x':0.5,\n",
    "        'xanchor': 'center',\n",
    "        'yanchor': 'top'})\n",
    "    \n",
    "    fig.update_layout(\n",
    "        scene = dict(\n",
    "        xaxis = dict(visible=False),\n",
    "        yaxis = dict(visible=False),\n",
    "        zaxis =dict(visible=False)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #check_num_channels_correct(chs_by_lobe, 'END_PLOT') #check we didnt change the original dict\n",
    "\n",
    "\n",
    "    # Add the button to have names show up on hover or always:\n",
    "    fig = switch_names_on_off(fig)\n",
    "\n",
    "    fig.update_traces(hoverlabel=dict(font=dict(size=10))) #TEXT SIZE set to 10 again. This works for the \"Show names on hover\" option, but not for \"Always show names\" option\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    qc_derivative = [QC_derivative(content=fig, name='Sensors_positions', content_type='plotly', description_for_user=\"Magnetometers names end with '1' like 'MEG0111'. Gradiometers names end with '2' and '3' like 'MEG0112', 'MEG0113'. \")]\n",
    "\n",
    "    return qc_derivative \n",
    "\n",
    "plot_sensors_3d_csv(sensors_csv_path = '/Volumes/M2_DATA/MEG_QC_stuff/data/openneuro/ds003483/derivatives/Meg_QC/sub-009/sub-009_ses-1_task-deduction_run-1_desc-Sensors_meg.tsv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\n",
    "df = df.reset_index()  # make sure indexes pair with number of rows\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(row['c1'], row['c2'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotting_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/Paris2020/sub-emptyroom/meg/sub-emptyroom_task-Paris5_meg.fif', allow_maxshield=True)\n",
    "#raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/LeerraumMD2016/sub-emptyroom/meg/sub-emptyroom_task-Magdeburg2_meg.fif')\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aligned Wave Shapes:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Generate two aligned wave shapes\n",
    "time = np.linspace(0, 1, 100)\n",
    "wave1 = np.sin(2 * np.pi * 2 * time)\n",
    "wave2 = np.sin(2 * np.pi * 2 * time)\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = np.correlate(wave1, wave2, mode='same')\n",
    "corr1 = pearsonr(wave1, wave2)\n",
    "print(corr1)\n",
    "\n",
    "# Plot the wave shapes and correlation\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(time, wave1)\n",
    "plt.title('Wave 1')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(time, wave2)\n",
    "plt.title('Wave 2')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Misaligned Wave Shapes:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate two misaligned wave shapes\n",
    "time = np.linspace(0, 1, 100)\n",
    "wave1 = np.sin(2 * np.pi * 2 * time)\n",
    "wave2 = np.sin(2 * np.pi * 2 * (time + 0.15))  # Shifted by 0.2 seconds\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = np.correlate(wave1, wave2, mode='same')\n",
    "corr2 = pearsonr(wave1, wave2)\n",
    "print(corr2)\n",
    "\n",
    "# Plot the wave shapes and correlation\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(time, wave1)\n",
    "plt.title('Wave 1')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(time, wave2)\n",
    "plt.title('Wave 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create wave shapes\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of points in each array\n",
    "num_points = 100\n",
    "\n",
    "# Create an array of time values\n",
    "t = np.linspace(0, 2*np.pi, num_points)\n",
    "\n",
    "# Define the amplitudes for the R-wave shapes\n",
    "amplitudes = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5]\n",
    "\n",
    "# Define the maximum time shift in seconds\n",
    "max_shift = 0.4\n",
    "\n",
    "# Create five arrays with R-wave shapes, shifted in time\n",
    "waves = []\n",
    "for i, amplitude in enumerate(amplitudes):\n",
    "    # Generate a random time shift within the maximum shift range\n",
    "    time_shift = np.random.uniform(-max_shift, max_shift)\n",
    "    \n",
    "    # Shift the time values\n",
    "    shifted_t = t + time_shift\n",
    "    \n",
    "    # Create the R-wave shape with the shifted time values\n",
    "    wave = np.exp(-shifted_t) * np.sin(4*shifted_t) * amplitude\n",
    "    #waves.append(wave)\n",
    "    waves.append(wave[::-1])\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "for i, wave in enumerate(waves):\n",
    "    fig.add_trace(go.Scatter(x=time, y=wave, name=f'Wave {i+1}'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have two arrays: array1 and array2\n",
    "array1 = waves[0]\n",
    "array2 = waves[5]\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Assuming you have two arrays: array1 and array2\n",
    "\n",
    "# Find peaks in both arrays\n",
    "peaks1, _ = find_peaks(array1)\n",
    "peaks2, _ = find_peaks(array2)\n",
    "\n",
    "# Calculate the time shift based on the peak positions\n",
    "time_shift = peaks1[0] - peaks2[0]\n",
    "\n",
    "# Shift array2 to align with array1\n",
    "aligned_array2 = np.roll(array2, time_shift)\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the array1 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(array1)), y=array1, name='Array 1'))\n",
    "\n",
    "# Add the array2 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(array2)), y=array2, name='Array 2'))\n",
    "\n",
    "# Add the aligned_array2 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(aligned_array2)), y=aligned_array2, name='Aligned Array 2'))\n",
    "\n",
    "# Set the layout\n",
    "fig.update_layout(title='Aligned Arrays using Peak Detection',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='Amplitude')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "array1 = waves[0]\n",
    "array2 = -waves[5]\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the array1 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(array1)), y=array1, name='Array 1'))\n",
    "\n",
    "# Add the array2 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(array2)), y=array2, name='Array 2'))\n",
    "\n",
    "# Set the layout\n",
    "fig.update_layout(title='Aligned Arrays using Peak Detection',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='Amplitude')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming you have two arrays: array1 and array2\n",
    "\n",
    "# Find peaks in array1\n",
    "peaks1, _ = find_peaks(array1)\n",
    "\n",
    "# Initialize variables for best alignment\n",
    "best_time_shift = 0\n",
    "best_correlation = -np.inf\n",
    "best_aligned_array2 = None\n",
    "\n",
    "# Try aligning array2 in both orientations\n",
    "for flip in [False, True]:\n",
    "    # Flip array2 if needed\n",
    "    #aligned_array2 = np.flip(array2) if flip else array2\n",
    "    aligned_array2 = -array2 if flip else array2\n",
    "\n",
    "    # Find peaks in aligned_array2\n",
    "    peaks2, _ = find_peaks(aligned_array2)\n",
    "\n",
    "    # Calculate the time shift based on the peak positions\n",
    "    time_shift = peaks1[0] - peaks2[0]\n",
    "\n",
    "    # Shift aligned_array2 to align with array1\n",
    "    aligned_array2 = np.roll(aligned_array2, time_shift)\n",
    "\n",
    "    # Calculate the correlation between array1 and aligned_array2\n",
    "    correlation = np.corrcoef(array1, aligned_array2)[0, 1]\n",
    "\n",
    "    # Update the best alignment if the correlation is higher\n",
    "    if correlation > best_correlation:\n",
    "        best_correlation = correlation\n",
    "        best_time_shift = time_shift\n",
    "        best_aligned_array2 = aligned_array2\n",
    "\n",
    "# Create the figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the array1 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(array1)), y=array1, name='Array 1'))\n",
    "\n",
    "# Add the array2 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(array2)), y=array2, name='Array 2'))\n",
    "\n",
    "# Add the best_aligned_array2 trace\n",
    "fig.add_trace(go.Scatter(x=np.arange(len(best_aligned_array2)), y=best_aligned_array2, name='Aligned Array 2'))\n",
    "\n",
    "# Set the layout\n",
    "fig.update_layout(title='Aligned Arrays with Flipped Second Array',\n",
    "                  xaxis_title='Time',\n",
    "                  yaxis_title='Amplitude')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "avg_ecg_epoch_data_nonflipped_limited_to_event = np.array(waves)\n",
    "\n",
    "max_values=np.max(np.abs(avg_ecg_epoch_data_nonflipped_limited_to_event), axis=1)\n",
    "print(max_values)\n",
    "max_values_ind=np.argsort(max_values)[::-1] \n",
    "print(max_values_ind)\n",
    "max_values_ind=max_values_ind[:5]\n",
    "\n",
    "chosen_5 = (avg_ecg_epoch_data_nonflipped_limited_to_event[max_values_ind])\n",
    "\n",
    "thresh_lvl_peakfinder = 5\n",
    "\n",
    "\n",
    "#get the highest peak for every channel:\n",
    "max_amplitude1 = []\n",
    "index_of_max_amplitude1=[]\n",
    "for ch_data in avg_ecg_epoch_data_nonflipped_limited_to_event:\n",
    "\n",
    "    thresh_mean=(max(ch_data) - min(ch_data)) / thresh_lvl_peakfinder\n",
    "    peak_locs_pos, _ = find_peaks(ch_data, prominence=thresh_mean)\n",
    "    peak_locs_neg, _ = find_peaks(-ch_data, prominence=thresh_mean)\n",
    "\n",
    "    all_peaks = np.concatenate((peak_locs_pos, peak_locs_neg))\n",
    "    print('all peaks', all_peaks)\n",
    "\n",
    "    #Find the peak with the maximal amplitude:\n",
    "\n",
    "    max_amplitude_peak = np.argmax(np.abs(ch_data[all_peaks]))\n",
    "\n",
    "    #now find the index of this point in the channel data:\n",
    "    index_of_max_amplitude1.append(all_peaks[max_amplitude_peak])\n",
    "\n",
    "    print('Index1', index_of_max_amplitude1)\n",
    "\n",
    "    #now find the magnitude of the data in this point:\n",
    "\n",
    "    max_amplitude1.append(ch_data[index_of_max_amplitude1[-1]])\n",
    "\n",
    "    \n",
    "\n",
    "# find 5 channels which have the highest peaks and get the locations of these peaks:\n",
    "highest_channels_sorted = np.argsort(max_amplitude1)[::-1] \n",
    "print(highest_channels_sorted)\n",
    "max_ind_of_chosen_5=highest_channels_sorted[:5]\n",
    "\n",
    "print(max_ind_of_chosen_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_of_max_amplitude2=[]\n",
    "for ch_data in max_ind_of_chosen_5:\n",
    "\n",
    "    thresh_mean=(max(ch_data) - min(ch_data)) / thresh_lvl_peakfinder\n",
    "    peak_locs_pos, _ = find_peaks(ch_data, prominence=thresh_mean)\n",
    "    peak_locs_neg, _ = find_peaks(-ch_data, prominence=thresh_mean)\n",
    "\n",
    "    all_peaks = np.concatenate((peak_locs_pos, peak_locs_neg))\n",
    "    print('all peaks', all_peaks)\n",
    "\n",
    "    #Find the peak with the maximal amplitude:\n",
    "\n",
    "    max_amplitude_peak = np.argmax(np.abs(ch_data[all_peaks]))\n",
    "\n",
    "\n",
    "    #6. Output the index of the point with the maximal amplitude:\n",
    "\n",
    "    index_of_max_amplitude1.append(all_peaks[max_amplitude_peak])\n",
    "    print('Index1', index_of_max_amplitude1)\n",
    "\n",
    "    if len(all_peaks)>1:\n",
    "        #7. Now find the second largest peak:\n",
    "        all_peaks_without_max = np.delete(all_peaks, max_amplitude_peak)\n",
    "\n",
    "        print('no max', all_peaks_without_max)\n",
    "\n",
    "        max_amplitude_peak = np.argmax(np.abs(ch_data[all_peaks_without_max]))\n",
    "\n",
    "\n",
    "        #6. Output the index of the point with the maximal amplitude:\n",
    "\n",
    "        index_of_max_amplitude2.append(all_peaks_without_max[max_amplitude_peak])\n",
    "        print('Index2', index_of_max_amplitude2)\n",
    "        \n",
    "    else:\n",
    "        index_of_max_amplitude2.append(np.nan)\n",
    "\n",
    "mean_index_of_max_amplitude1 = np.nanmean(index_of_max_amplitude1)\n",
    "\n",
    "# If in more than a half of cases there was no second biggest peak found, skip it and assign t) as first peak:\n",
    "non_zero_count = np.count_nonzero(index_of_max_amplitude2)\n",
    "percentage = (non_zero_count/len(index_of_max_amplitude2)) * 100\n",
    "\n",
    "if percentage < 50:\n",
    "    t0_peak = int(mean_index_of_max_amplitude1)\n",
    "else:\n",
    "    mean_index_of_max_amplitude2 = np.nanmean(index_of_max_amplitude2)\n",
    "    #Now out of them set the first peak (according to time) as t0.\n",
    "    t0_peak = int(np.nanmin([mean_index_of_max_amplitude1, mean_index_of_max_amplitude2]))\n",
    "\n",
    "\n",
    "print('mean_ind1', mean_index_of_max_amplitude1)\n",
    "print('mean_ind2', mean_index_of_max_amplitude2)\n",
    "\n",
    "\n",
    "print(t0_peak)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([5, 2, 9, 1, 7, 3])\n",
    "\n",
    "# Get the indices that would sort the array in ascending order\n",
    "sorted_indices = np.argsort(arr)\n",
    "\n",
    "# Index of the largest value\n",
    "largest_index = sorted_indices[-1]\n",
    "\n",
    "# Index of the second largest value\n",
    "second_largest_index = sorted_indices[-2]\n",
    "\n",
    "print(\"Index of the largest value:\", largest_index)\n",
    "print(\"Index of the second largest value:\", second_largest_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from IPython.display import display\n",
    "\n",
    "# Load the MEG data\n",
    "raw=mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/openneuro/ds004107/sub-mind002/ses-01/meg/sub-mind002_ses-01_task-auditory_meg.fif', preload=True)\n",
    "\n",
    "display(raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the EOG channel names\n",
    "eog_channels = ['EOG 061', 'EOG 062']\n",
    "\n",
    "# extract the data of 2 EOG channels\n",
    "eog_data = raw.copy().pick_channels(eog_channels).get_data()\n",
    "\n",
    "print(eog_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the data with plotly:\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "x_values = raw.times\n",
    "\n",
    "# Add a trace for the first subplot\n",
    "fig.add_trace(go.Scatter(x=x_values, y=eog_data[0], mode='lines', name='EOG 1'), row=1, col=1)\n",
    "\n",
    "# Add a trace for the second subplot\n",
    "fig.add_trace(go.Scatter(x=x_values, y=eog_data[1], mode='lines', name='EOG 2'), row=2, col=1)\n",
    "\n",
    "# Update the layout\n",
    "fig.update_layout(title='EOG Data', xaxis_title='Time (s)', yaxis_title='Amplitude')\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# create two arrays\n",
    "array1 = np.array([1, 2, 3, 4, 5])\n",
    "array2 = np.array([6, 7, 8, 9, 10])\n",
    "\n",
    "# stack the arrays horizontally\n",
    "stacked = np.stack((array1, array2), axis=0)\n",
    "\n",
    "display(stacked)\n",
    "\n",
    "# calculate the covariance matrix\n",
    "covariance_matrix = np.cov(stacked)\n",
    "\n",
    "print(covariance_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Load the MEG data\n",
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/openneuro/ds003703/sub-a68d5xp5/meg/sub-a68d5xp5_task-listeningToSpeech_run-01_meg.fif')\n",
    "#raw=mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/openneuro/ds004107/sub-mind002/ses-01/meg/sub-mind002_ses-01_task-auditory_meg.fif', preload=True)\n",
    "\n",
    "# Select the EOG channels\n",
    "eog_channels = mne.pick_types(raw.info, meg=False, eeg=False, stim=False, eog=True)\n",
    "\n",
    "# Get the names of the EOG channels\n",
    "eog_channel_names = [raw.ch_names[ch] for ch in eog_channels]\n",
    "\n",
    "print('EOG channel names:', eog_channel_names)\n",
    "\n",
    "eog_events = mne.preprocessing.find_eog_events(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "picks_ECG = mne.pick_types(raw.info, ecg=True)\n",
    "ecg_ch_name = [raw.info['chs'][name]['ch_name'] for name in picks_ECG]\n",
    "\n",
    "arr=raw.get_data(picks=ecg_ch_name)[0] \n",
    "height = np.mean(arr) + 1 * np.std(arr)\n",
    "fs=raw.info['sfreq']\n",
    "peaks, _ = find_peaks(arr, height=height, distance=round(0.5 * fs)) #assume there are no peaks within 0.5 seconds from each other.\n",
    "ecg_events = peaks/fs\n",
    "\n",
    "# Define the time window of interest\n",
    "time_window = [0.2, 0.2]  # in seconds\n",
    "tmin=-0.2\n",
    "tmax=0.2\n",
    "\n",
    "# Convert time window to samples\n",
    "sfreq = 1000  # sampling frequency of your data\n",
    "time_window_samples = np.round(np.array(time_window) * sfreq).astype(int)\n",
    "print('samples', time_window_samples)\n",
    "\n",
    "# Initialize an empty array to store the extracted epochs\n",
    "epochs = np.zeros((len(peaks), int((tmax-tmin)*sfreq)))\n",
    "\n",
    "print('HERE')\n",
    "print(arr)\n",
    "print(epochs)\n",
    "\n",
    "# Loop through each ECG event and extract the corresponding epoch\n",
    "for i, event in enumerate(peaks):\n",
    "    start = event - time_window_samples[0]\n",
    "    start = np.round(event + tmin*sfreq).astype(int)\n",
    "    end = event + time_window_samples[1]\n",
    "    end= np.round(event + tmax*sfreq).astype(int)\n",
    "    epochs[i, :] = arr[start:end]\n",
    "\n",
    "#average all epochs:\n",
    "avg_ecg=np.mean(epochs, axis=0)\n",
    "\n",
    "#print average ecg with plotly:\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "#create time vector based on time window and sampling frequency:\n",
    "times= np.arange(tmin, tmax, 1/sfreq)\n",
    "fig.add_trace(go.Scatter(x=times, y=avg_ecg, mode='lines', name='ECG'))\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detect the R-wave peaks in the filtered ECG channel data\n",
    "r_peaks, ch_ecg, pulse, ecg_data_rec = mne.preprocessing.find_ecg_events(raw, return_ecg=True)\n",
    "print(ecg_data_rec)\n",
    "\n",
    "#plot the ECG data with plotly:\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "times=[t for t in range(len(ecg_data_rec[0]))]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=times, y=ecg_data_rec[0], mode='lines', name='ECG'))\n",
    "fig.update_layout(title='ECG data', xaxis_title='Time (s)', yaxis_title='ECG (mV)')\n",
    "fig.show()\n",
    "\n",
    "# Calculate the time difference between each R-wave peak and the first R-wave peak\n",
    "r_wave_epochs = (r_peaks - r_peaks[0]) / raw.info['sfreq']\n",
    "print('r_wave_epochs', r_wave_epochs)\n",
    "\n",
    "# Calculate the average R-wave epoch\n",
    "avg_r_wave_epoch = np.mean(r_wave_epochs)\n",
    "print('avg_r_wave_epoch', avg_r_wave_epoch)\n",
    "\n",
    "if ecg_ch_name:\n",
    "    # Extract the ECG channel data\n",
    "    ecg_data, times = raw.get_data(ecg_ch_name, return_times=True)\n",
    "    ecg_data2=ecg_data_rec\n",
    "else:\n",
    "    ecg_data=ecg_data_rec\n",
    "\n",
    "\n",
    "\n",
    "# Use the average R-wave epoch to extract a segment of data from the ECG channel\n",
    "avg_r_wave_data = ecg_data[:, int(avg_r_wave_epoch * raw.info['sfreq']) : int((avg_r_wave_epoch + 0.2) * raw.info['sfreq'])]\n",
    "avg_r_wave_data2 = ecg_data2[:, int(avg_r_wave_epoch * raw.info['sfreq']) : int((avg_r_wave_epoch + 0.2) * raw.info['sfreq'])]\n",
    "\n",
    "#plot the average R-wave epoch with plotly:\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "times=[t/raw.info['sfreq'] for t in range(len(avg_r_wave_data[0]))]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=times, y=avg_r_wave_data[0], mode='lines', name='ECG'))\n",
    "fig.add_trace(go.Scatter(x=times, y=avg_r_wave_data2[0], mode='lines', name='ECG2'))\n",
    "fig.update_layout(title='Average R-wave epoch', xaxis_title='Time (s)', yaxis_title='ECG (mV)')\n",
    "fig.show()\n",
    "\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([t/raw.info['sfreq'] for t in range(len(avg_r_wave_data[0]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_data\n",
    "avg_r_wave_epoch * raw.info['sfreq']\n",
    "r_peaks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_r_wave_data = ecg_data[:, int(avg_r_wave_epoch * raw.info['sfreq']) : int((avg_r_wave_epoch + 0.2) * raw.info['sfreq'])]\n",
    "avg_r_wave_data2 = ecg_data2[:, int(avg_r_wave_epoch * raw.info['sfreq']) : int((avg_r_wave_epoch + 0.2) * raw.info['sfreq'])]\n",
    "\n",
    "#plot the average R-wave epoch with plotly:\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "times=[t/raw.info['sfreq'] for t in range(len(avg_r_wave_data[0]))]\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=times, y=avg_r_wave_data[0], mode='lines', name='ECG'))\n",
    "fig.add_trace(go.Scatter(x=times, y=avg_r_wave_data2[0], mode='lines', name='ECG2'))\n",
    "fig.update_layout(title='Average R-wave epoch', xaxis_title='Time (s)', yaxis_title='ECG (mV)')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Generate two waves\n",
    "wave1 = np.array([1, 2, 3, 4, 5])\n",
    "wave2 = np.array([2, 4, 6, 8, 10])\n",
    "\n",
    "# Calculate the Pearson correlation coefficient and p-value\n",
    "corr_coef, p_value = pearsonr(wave1, wave2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Pearson correlation coefficient:\", corr_coef)\n",
    "print(\"p-value:\", p_value)\n",
    "\n",
    "\n",
    "#plot both waves with plotly:\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=wave1, mode='lines', name='wave1'))\n",
    "fig.add_trace(go.Scatter(y=wave2, mode='lines', name='wave2'))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "lobe_colors = {\n",
    "        'Left Frontal': '#1f77b4',\n",
    "        'Right Frontal': '#ff7f0e',\n",
    "        'Left Temporal': '#2ca02c',\n",
    "        'Right Temporal': '#9467bd',\n",
    "        'Left Parietal': '#e377c2',\n",
    "        'Right Parietal': '#d62728',\n",
    "        'Left Occipital': '#bcbd22',\n",
    "        'Right Occipital': '#17becf'}\n",
    "\n",
    "print(random.choice(list(lobe_colors.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import gaussian_filter\n",
    "import numpy as np\n",
    "\n",
    "# Generate some noisy wave data\n",
    "x = np.linspace(0, 2*np.pi, 100)\n",
    "y = np.sin(x) + np.random.normal(0, 0.5, 100)\n",
    "\n",
    "# Apply Gaussian smoothing with a sigma of 2\n",
    "y_smooth = gaussian_filter(y, sigma=4)\n",
    "\n",
    "# Plot the original and smoothed waves\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x, y, label='Noisy wave')\n",
    "plt.plot(x, y_smooth, label='Smoothed wave')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "# create sample data\n",
    "df = pd.DataFrame({'values': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]}, index=['A']*10)\n",
    "df = df.T\n",
    "print(df)\n",
    "\n",
    "# create box plot trace\n",
    "box_trace = go.Box(x=df.iloc[0], orientation='h')\n",
    "#box_trace = go.Box(x=df['values'], y=df.index, orientation='h', name='')\n",
    "\n",
    "fig = go.Figure(data=box_trace)\n",
    "\n",
    "for col in df.columns:\n",
    "    fig.add_trace(go.Scatter(x=df[col], name=col))\n",
    "\n",
    "# for v in df['values']:\n",
    "#     #fig.add_trace(go.Scatter(x=df['values'], y=df.index, mode='markers', marker=dict(size=5, color='yellow'), name='Scatter Plot', hovertext=df.index))\n",
    "#     fig.add_trace(go.Scatter(x=[v], y=['A'], mode='markers', marker=dict(size=5, color='yellow'), name='Scatter Plot', hovertext=df.index))\n",
    "\n",
    "# plot figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add box plot trace\n",
    "fig.add_trace(go.Box(x=[1, 2, 3, 4, 5]))\n",
    "\n",
    "# Add horizontal line at y=0\n",
    "fig.update_layout(\n",
    "    shapes=[\n",
    "        dict(\n",
    "            type='line',\n",
    "            yref='y',\n",
    "            y0=0,\n",
    "            y1=0,\n",
    "            xref='paper',\n",
    "            x0=0,\n",
    "            x1=1\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create example dataset\n",
    "np.random.seed(123)\n",
    "std_val = pd.DataFrame({'Group': ['A', 'A', 'B', 'B', 'C', 'C'],\n",
    "                     'Value': np.random.normal(size=6)})\n",
    "\n",
    "# Create box plot with custom marker colors\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(x=std_val['Group'], y=std_val['Value'], name='Value',\n",
    "                     marker=dict(color='red')))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(title='Box plot with custom marker colors',\n",
    "                  xaxis_title='Group', yaxis_title='Value')\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "\n",
    "# create a box plot with custom marker color\n",
    "trace = go.Box(\n",
    "    y=[1, 2, 3, 4, 5],\n",
    "    marker=dict(\n",
    "        color='blue'\n",
    "    )\n",
    ")\n",
    "\n",
    "# create a figure and add the box plot trace\n",
    "fig = go.Figure(data=[trace])\n",
    "\n",
    "# show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This here is to save all the average ECG/EOG data into a pickle file, so I can test difefrent wave detection algorythms on them without running the pipeline again\n",
    "\n",
    "import pickle \n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# open a file in write binary mode\n",
    "with open(\"avg_ecg.pkl\", \"wb\") as f:\n",
    "    # dump the list of objects into the file\n",
    "    pickle.dump(avg_ecg, f)\n",
    "\n",
    "with open(\"avg_eog.pkl\", \"wb\") as f:\n",
    "    # dump the list of objects into the file\n",
    "    pickle.dump(avg_eog, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This here is to open the pickle files from above and plot the data\n",
    "\n",
    "import pickle \n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# open the file in read binary mode\n",
    "with open(\"avg_ecg0.pkl\", \"rb\") as f:\n",
    "    # load the list of objects from the file\n",
    "    eog_list = pickle.load(f)\n",
    "\n",
    "print(eog_list[0])\n",
    "\n",
    "sfreq=1000\n",
    "t = np.round(np.arange(-0.4, 0.4+1/sfreq, 1/sfreq), 3) #yes, you need to round\n",
    "fig0=go.Figure()\n",
    "for x in range(0, len(eog_list)):\n",
    "    fig_temp=eog_list[x].plot_epoch_and_peak(t, 'Channels affected by ECG artifact: ', 'mag', fig0)\n",
    "    for trace in fig_temp['data']:\n",
    "        fig0.add_trace(trace)\n",
    "\n",
    "fig0.update_layout(\n",
    "    yaxis = dict(\n",
    "            showexponent = 'all',\n",
    "            exponentformat = 'e')) \n",
    "fig0.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for EOG\n",
    "\n",
    "import pickle \n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "\n",
    "# open the file in read binary mode\n",
    "with open(\"avg_eog0.pkl\", \"rb\") as f:\n",
    "    # load the list of objects from the file\n",
    "    eog_list = pickle.load(f)\n",
    "\n",
    "sfreq=1000\n",
    "t = np.round(np.arange(-0.4, 0.4+1/sfreq, 1/sfreq), 3) #yes, you need to round\n",
    "fig0=go.Figure()\n",
    "for x in range(0, len(eog_list)):\n",
    "    fig_temp=eog_list[x].plot_epoch_and_peak(t, 'Channels affected by ECG artifact: ', 'mag')\n",
    "    for trace in fig_temp['data']:\n",
    "        fig0.add_trace(trace)\n",
    "\n",
    "fig0.update_layout(\n",
    "    yaxis = dict(\n",
    "            showexponent = 'all',\n",
    "            exponentformat = 'e')) \n",
    "fig0.show()\n",
    "\n",
    "#Now apply the gaussia filter to each trace and plot result in the same figure:\n",
    "fig0_new=deepcopy(fig0)\n",
    "for trace in fig0_new['data']:\n",
    "    y=trace['y']\n",
    "    y_smooth = gaussian_filter(y, sigma=10)\n",
    "    trace['y']=y_smooth\n",
    "\n",
    "fig0_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/LeerraumAarhus2017/sub-emptyroom/meg/sub-emptyroom_task-Aarhus_meg.fif', preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show sensor posiions using mne:\n",
    "\n",
    "import mne\n",
    "\n",
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/LeerraumAarhus2017/sub-emptyroom/meg/sub-emptyroom_task-Aarhus_meg.fif', preload=True)\n",
    "\n",
    "mne.viz.plot_sensors(raw.info, kind='topomap', ch_type='grad', show_names=True, ch_groups='position')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT SENSORS IN 2D with plotly\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import mne\n",
    "\n",
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/LeerraumAarhus2017/sub-emptyroom/meg/sub-emptyroom_task-Aarhus_meg.fif', preload=True)\n",
    "\n",
    "\n",
    "mag_ch_names = raw.copy().pick_types(meg='mag').ch_names if 'mag' in raw else None\n",
    "grad_ch_names = raw.copy().pick_types(meg='grad').ch_names if 'grad' in raw else None\n",
    "channels_objs = {'mag': mag_ch_names, 'grad': grad_ch_names}\n",
    "\n",
    "# Get the sensor locations\n",
    "sensor_locs = raw.info['chs']\n",
    "#print(sensor_locs)\n",
    "#coords_mag=[loc['loc'][:2] for loc in sensor_locs]\n",
    "coords_mag=[loc['loc'] for loc in sensor_locs if loc['ch_name'] in mag_ch_names]\n",
    "#print(len(coords), coords)\n",
    "print(len(coords_mag), coords_mag)\n",
    "\n",
    "x = [r[0] for r in coords_mag]\n",
    "y = [r[1] for r in coords_mag]\n",
    "#x, y, z = [loc['loc'][:3] for loc in sensor_locs]\n",
    "names = [loc['ch_name'] for loc in sensor_locs if loc['ch_name'] in mag_ch_names]\n",
    "kinds= [loc['kind'] for loc in sensor_locs]\n",
    "print(kinds)\n",
    "\n",
    "# Create a scatter plot of the sensor locations\n",
    "fig = go.Figure(data=go.Scatter(x=x, y=y, mode='markers', text=names))\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1000,\n",
    "    height=1000)\n",
    "\n",
    "# Set the plot title and axis labels\n",
    "fig.update_layout(title='MEG Sensor Locations', xaxis_title='X', yaxis_title='Y')\n",
    "\n",
    "# Add a circle shape to the plot to show the position of the head\n",
    "fig.update_layout(\n",
    "    shapes=[\n",
    "        dict(\n",
    "            type='circle',\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            x0=-0.1,\n",
    "            y0=-0.1,\n",
    "            x1=0.1,\n",
    "            y1=0.12,\n",
    "            line=dict(color='red', width=2),\n",
    "            opacity=0.5\n",
    "        ),\n",
    "        dict(\n",
    "            type='line',\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            x0=[0, -0.02],\n",
    "            y0=[0.1, 0.08],\n",
    "            line=dict(color='black', width=2)\n",
    "        ),\n",
    "        dict(\n",
    "            type='line',\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            x0=[0, 0.02],\n",
    "            y0=[0.1, 0.08],\n",
    "            line=dict(color='black', width=2)\n",
    "        ),\n",
    "        dict(\n",
    "            type='line',\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            x0=[-0.02, 0.02],\n",
    "            y0=[0.08, 0.08],\n",
    "            line=dict(color='black', width=2))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Show the plot \n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/LeerraumAarhus2017/sub-emptyroom/meg/sub-emptyroom_task-Aarhus_meg.fif', preload=True)\n",
    "\n",
    "#PLOT 3 D\n",
    "\n",
    "def switch_names_on_off(fig):\n",
    "    # Define the buttons\n",
    "    buttons = [\n",
    "    dict(label='Show channel names when hovering',\n",
    "         method='update',\n",
    "         args=[{'mode': 'markers'}]),\n",
    "    dict(label='Always show channel names',\n",
    "         method='update',\n",
    "         args=[{'mode': 'markers+text'}])\n",
    "    ]\n",
    "\n",
    "    # Add the buttons to the layout\n",
    "    fig.update_layout(updatemenus=[dict(type='buttons',\n",
    "                                        showactive=True,\n",
    "                                        buttons=buttons)])\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "# Extract the sensor locations and names for magnetometers\n",
    "mag_locs = raw.copy().pick_types(meg='mag').info['chs']\n",
    "mag_pos = [ch['loc'][:3] for ch in mag_locs]\n",
    "mag_names = [ch['ch_name'] for ch in mag_locs]\n",
    "\n",
    "# Create the magnetometer plot with markers only\n",
    "\n",
    "mag_fig = go.Figure(data=[go.Scatter3d(x=[pos[0] for pos in mag_pos],\n",
    "                                       y=[pos[1] for pos in mag_pos],\n",
    "                                       z=[pos[2] for pos in mag_pos],\n",
    "                                       mode='markers',\n",
    "                                       marker=dict(size=5),\n",
    "                                       text=mag_names,\n",
    "                                       hovertemplate='%{text}')],\n",
    "                                       layout=go.Layout(width=1000, height=1000))\n",
    "\n",
    "mag_fig.update_layout(title='Magnetometers')\n",
    "\n",
    "mag_fig = switch_names_on_off(mag_fig)\n",
    "mag_fig.show()\n",
    "\n",
    "\n",
    "\n",
    "# Extract the sensor locations and names for gradiometers\n",
    "grad_locs = raw.copy().pick_types(meg='grad').info['chs']\n",
    "grad_pos = [ch['loc'][:3] for ch in grad_locs]\n",
    "grad_names = [ch['ch_name'] for ch in grad_locs]\n",
    "\n",
    "#since grads have 2 sensors located in the same spot - need to put their names together to make pretty plot labels:\n",
    "\n",
    "grad_pos_together = []\n",
    "grad_names_together = []\n",
    "\n",
    "for i in range(len(grad_pos)-1):\n",
    "    if all(x == y for x, y in zip(grad_pos[i], grad_pos[i+1])):\n",
    "        grad_pos_together += [grad_pos[i]]\n",
    "        grad_names_together += [grad_names[i]+', '+grad_names[i+1]]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Add both sets of gradiometer positions to the plot:\n",
    "grad_fig = go.Figure(data=[go.Scatter3d(x=[pos[0] for pos in grad_pos_together],\n",
    "                                        y=[pos[1] for pos in grad_pos_together],\n",
    "                                        z=[pos[2] for pos in grad_pos_together],\n",
    "                                        mode='markers',\n",
    "                                        marker=dict(size=5),\n",
    "                                        text=grad_names_together,\n",
    "                                        hovertemplate='%{text}')],\n",
    "                                        layout=go.Layout(width=1000, height=1000))\n",
    "\n",
    "grad_fig.update_layout(title='Gradiometers')\n",
    "\n",
    "\n",
    "# Add the button to have names show up on hover or always:\n",
    "grad_fig = switch_names_on_off(grad_fig)\n",
    "\n",
    "# Show the plots\n",
    "\n",
    "grad_fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MUSCLE ARIFACTS IN EMPTYROOM DATA:\n",
    "# Discussed with Andreas:\n",
    "# We can see very high muscle scores at the very beginning and end of the empty room recording\n",
    "# Are these real muscle artifacts or filtering errors?\n",
    "# Cut out 1st second of the data where they are visible.\n",
    "# make Fourier transform of the 1st second and see if there are high amplitudes visible for the muscle frequencies - nope \n",
    "# most likely this is filtering.\n",
    "# next, follow the MNE steps for muscle artifact detection: they use first filtering at 11--140 hz, then Hilbert\n",
    "# plotted raw data after the applied filter, and Hilbert - see cut artifacts in the beginning and end. (WJY arw we sure it  s not hilbert?)\n",
    "# then, tried to only filter - very noisy data. but most likely the filtering is the source. Because of the cut-off in the beginning and end.\n",
    "# Solutions: zero padding in the beginning and end before filtering, which will be cut off after. But may still create a jump while filtering and keep the artifact.\n",
    "# Better: add 2s of dummy data at the beginning and end of the recording, and then crop it out (the data added should be mirrored). This will not create a jump in the filtering.\n",
    "# Tried\n",
    "\n",
    "# Problem found! 2 problems: \n",
    "# 1st: The main artifact is actually introduced by filtering power lines. filtering the data at 150 Hz (harmonics) clearly creates this artifact.\n",
    "# Removed that and any other filtering over the range of muscle freqs, since we don't need them anyways. (over 140 Hz)\n",
    "# 2nd: Still some artifact is present in the beginning and end of the recording. For this attach mirrored data on both ends.\n",
    "# Then detect muscle, then cut the resulting scores away for the attached period.\n",
    "# There will be still some very minimal artifact at the beginning/end of this attachment - probably due to the attachment itself: the mirrored data is still not a normally shaped signal.\n",
    "# See example in cell above of how all 3 option look: oroginal, with attached data and with attached and cut away.\n",
    "\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import mne\n",
    "\n",
    "raw = mne.io.read_raw_fif('/Volumes/M2_DATA/MEG_QC_stuff/data/Jochem/LeerraumAarhus2017/sub-emptyroom/meg/sub-emptyroom_task-Aarhus_meg.fif', preload=True)\n",
    "\n",
    "#%matplotlib qt\n",
    "raw_first = raw.copy().crop(tmin=0, tmax=1)\n",
    "#raw_first.plot()\n",
    "\n",
    "\n",
    "std_val=raw_first.get_data()\n",
    "\n",
    "window = np.hanning(std_val.shape[-1])*std_val\n",
    "\n",
    "window\n",
    "\n",
    "freqs = np.fft.rfftfreq(window.shape[-1], 1/raw.info['sfreq'])\n",
    "\n",
    "components = np.fft.fft(window, axis=-1)\n",
    "\n",
    "components.shape\n",
    "\n",
    "fig = go.Figure()\n",
    "for ch in range(15, 250):\n",
    "    fig.add_trace(go.Scatter(x=freqs, y = np.abs(components[ch, 0:500])))\n",
    "fig.add_trace(go.Scatter(x=freqs, y = np.abs(components[0, 0:500])))\n",
    "#from mne annotate_muscle_zscore:\n",
    "from scipy.stats import zscore\n",
    "from scipy.ndimage import label\n",
    "\n",
    "filter_freq=(110, 140)\n",
    "legend_category = 'mag'\n",
    "\n",
    "raw_copy = raw_first.copy()\n",
    "raw_copy.load_data()\n",
    "\n",
    "if legend_category is None:\n",
    "    raw_ch_type = raw_copy.get_channel_types()\n",
    "    if 'mag' in raw_ch_type:\n",
    "        legend_category = 'mag'\n",
    "    elif 'grad' in raw_ch_type:\n",
    "        legend_category = 'grad'\n",
    "    elif 'eeg' in raw_ch_type:\n",
    "        legend_category = 'eeg'\n",
    "    else:\n",
    "        raise ValueError('No M/EEG channel types found, please specify a'\n",
    "                            ' ch_type or provide M/EEG sensor data')\n",
    "\n",
    "if legend_category in ('mag', 'grad'):\n",
    "    raw_copy.pick_types(meg=legend_category, ref_meg=False)\n",
    "else:\n",
    "    legend_category = {'meg': False, legend_category: True}\n",
    "    raw_copy.pick_types(**legend_category)\n",
    "\n",
    "#raw_copy.filter(filter_freq[0], filter_freq[1], fir_design='firwin',\n",
    "#                pad=\"reflect_limited\")\n",
    "\n",
    "hilb_applied=raw_copy.apply_hilbert(envelope=True)\n",
    "hilb_applied.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the list of values into a NumPy array\n",
    "values = np.array([1, 2, 1.8, 2.5, 3, 3.5, 4, 3.8, 5, 4, 2, 2.1, 1, 0, 2, 4, 6])\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import butter, filtfilt, savgol_filter, find_peaks\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Generate a noisy wave shape\n",
    "t = np.linspace(0, 10, 1000)\n",
    "y = np.sin(t) + 0.5*np.random.randn(len(t))\n",
    "\n",
    "data = np.random.randn(1000) #no wave shape\n",
    "# Load the noisy wave data into a NumPy array\n",
    "wave_data = data\n",
    "\n",
    "# Create a Plotly figure\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t, y=wave_data, mode='lines', name='Noisy Wave'))\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Amplitude', title='Noisy Wave Shape')\n",
    "fig.show()\n",
    "\n",
    "# Apply a low-pass filter to remove high-frequency noise\n",
    "b, a = butter(5, 0.1, 'low')\n",
    "filtered_data = filtfilt(b, a, wave_data)\n",
    "\n",
    "# plot filtered data\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t, y=filtered_data, mode='lines', name='Filtered Wave'))\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Amplitude', title='Filtered Wave Shape')\n",
    "fig.show()\n",
    "\n",
    "# Apply a Savitzky-Golay filter to further reduce noise and extract the underlying wave shape\n",
    "#smoothed_data = savgol_filter(wave_data, window_length=int(len(wave_data)/4), polyorder=3)\n",
    "smoothed_data = savgol_filter(data, window_length=100, polyorder=3)\n",
    "\n",
    "# Identify the shape of the wave using peak detection or curve fitting\n",
    "# For example, you can use the `scipy.signal.find_peaks` function to detect peaks in the smoothed data\n",
    "#peaks, _ = find_peaks(smoothed_data, height=0.5*np.max(smoothed_data))\n",
    "peaks, _ = find_peaks(smoothed_data)\n",
    "\n",
    "# plot smoothed data\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=t, y=smoothed_data, mode='lines', name='Smoothed Wave'))\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Amplitude', title='Smoothed Wave Shape')\n",
    "fig.add_trace(go.Scatter(x=t[peaks], y=smoothed_data[peaks], mode='markers', name='Peaks'))\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Generate a noisy wave shape\n",
    "t = np.linspace(0, 10, 1000)\n",
    "y = np.sin(t) + 0.5*np.random.randn(len(t))\n",
    "\n",
    "# Find the peaks in the wave\n",
    "peaks, _ = find_peaks(y)\n",
    "\n",
    "# Count the number of peaks\n",
    "num_peaks = len(peaks)\n",
    "\n",
    "# Create a Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add the noisy wave shape to the figure\n",
    "fig.add_trace(go.Scatter(x=t, y=y, mode='lines', name='Noisy Wave'))\n",
    "\n",
    "# Add the peaks to the figure\n",
    "fig.add_trace(go.Scatter(x=t[peaks], y=y[peaks], mode='markers', name='Peaks'))\n",
    "\n",
    "# Add axis labels and a title\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Amplitude', title='Noisy Wave Shape')\n",
    "\n",
    "# Show the figure and print the number of peaks\n",
    "fig.show()\n",
    "print(f'The wave has {num_peaks} crest(s).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "picks_EOG = mne.pick_types(raw.info, eog=True)\n",
    "eog_ch_name = [raw.info['chs'][name]['ch_name'] for name in picks_EOG]\n",
    "eog_ch_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_psd = [2.93686870e-12, 5.37336497e-13, 2.34749324e-13, 1.70403629e-13\n",
    ", 1.42868936e-13, 1.10614848e-13, 1.01586902e-13, 9.41699507e-14\n",
    ", 8.41904711e-14, 7.56254639e-14, 6.98933286e-14, 6.47116338e-14\n",
    ", 5.37107007e-14, 5.42174045e-14, 4.78692577e-14, 4.36476164e-14\n",
    ", 3.69272073e-14, 3.81479068e-14, 4.07614720e-14, 3.71683505e-14\n",
    ", 3.74843265e-14, 3.57210926e-14, 3.99173535e-14, 4.87143053e-14\n",
    ", 4.20066645e-14, 3.84896719e-14, 3.13482998e-14, 2.86289627e-14\n",
    ", 2.82586165e-14, 2.71780036e-14, 2.48692250e-14, 2.71251350e-14\n",
    ", 2.79561808e-14, 2.72047767e-14, 2.79637330e-14, 2.55955578e-14\n",
    ", 2.53291180e-14, 2.01500680e-14, 2.12080778e-14, 2.18529602e-14\n",
    ", 2.12775368e-14, 2.14334140e-14, 2.18751322e-14, 1.97884378e-14\n",
    ", 1.89952388e-14, 1.79404586e-14, 2.01555022e-14, 2.21105500e-14\n",
    ", 1.87897255e-14, 1.95585625e-14, 2.07067862e-14, 2.15786185e-14\n",
    ", 1.78539522e-14, 1.89461927e-14, 1.83714513e-14, 1.91272285e-14\n",
    ", 1.81790918e-14, 1.51935998e-14, 1.55331746e-14, 1.37627320e-14\n",
    ", 1.37333973e-14, 1.47261781e-14, 1.35323358e-14, 1.23234074e-14\n",
    ", 1.26296023e-14, 1.39794705e-14, 1.32391885e-14, 1.33172509e-14\n",
    ", 1.40752537e-14, 1.35291881e-14, 1.46771014e-14, 1.57039580e-14\n",
    ", 1.76870590e-14, 2.11409680e-14, 2.66470647e-14, 2.09066429e-14\n",
    ", 1.68226688e-14, 1.63034232e-14, 1.32317697e-14, 1.20372472e-14\n",
    ", 1.10395275e-14, 1.17336558e-14, 1.12817157e-14, 1.31068881e-14\n",
    ", 1.36940739e-14, 1.48016686e-14, 1.35999052e-14, 1.56644411e-14\n",
    ", 1.51726149e-14, 1.95274934e-14, 1.84669709e-14, 1.89443054e-14\n",
    ", 1.82544652e-14, 1.92617658e-14, 1.80902967e-14, 2.17239287e-14\n",
    ", 2.67917043e-14, 4.45194367e-14, 2.01857457e-12, 4.40594585e-12\n",
    ", 1.74602155e-12, 4.30562941e-14, 2.53461498e-14, 1.87278757e-14\n",
    ", 1.54232144e-14, 1.67924147e-14, 1.22749773e-14, 1.25017017e-14\n",
    ", 1.22475994e-14, 1.02921463e-14, 1.07700101e-14, 1.02658035e-14\n",
    ", 9.54949869e-15, 9.84280694e-15, 8.88745310e-15, 9.02206922e-15\n",
    ", 8.47210049e-15, 8.64491709e-15, 1.32254861e-14, 1.89573615e-14\n",
    ", 1.23079196e-14, 8.62994931e-15, 8.12535185e-15, 8.01035318e-15\n",
    ", 7.53220890e-15, 8.02056256e-15, 7.90231409e-15, 7.63270083e-15\n",
    ", 7.93212379e-15, 7.28368608e-15, 7.59772607e-15, 7.26136429e-15\n",
    ", 7.86504197e-15, 7.36256360e-15, 7.02847343e-15, 7.08620266e-15\n",
    ", 6.86068169e-15, 7.21792433e-15, 7.28674098e-15, 6.88181371e-15\n",
    ", 6.78751472e-15, 6.59002904e-15, 6.74850515e-15, 6.53743454e-15\n",
    ", 6.70834535e-15, 6.72520433e-15, 6.78371437e-15, 6.70420118e-15\n",
    ", 6.97442862e-15, 7.26767528e-15, 6.79388360e-15, 6.83101277e-15\n",
    ", 6.90684197e-15, 6.45716620e-15, 6.66190889e-15, 6.49304182e-15\n",
    ", 6.38068712e-15, 6.29160702e-15, 5.92354089e-15, 6.33890242e-15\n",
    ", 6.33787606e-15, 5.76688121e-15, 6.31821916e-15, 6.34536916e-15\n",
    ", 6.51250512e-15, 6.43164190e-15, 6.46530769e-15, 6.44724883e-15\n",
    ", 7.48305304e-15, 7.50925230e-15, 6.28419317e-15, 6.33908319e-15\n",
    ", 5.86954984e-15, 6.54561206e-15, 6.08872456e-15, 6.40874736e-15\n",
    ", 5.95870142e-15, 6.13488554e-15, 5.83721527e-15, 5.87793931e-15\n",
    ", 5.81207088e-15, 5.98748087e-15, 5.94551525e-15, 5.75415575e-15\n",
    ", 5.66968278e-15, 6.11006036e-15, 5.72066372e-15, 5.96629716e-15\n",
    ", 5.80372053e-15, 5.75583336e-15, 5.84628922e-15, 5.63642362e-15\n",
    ", 5.34942930e-15, 5.75920960e-15, 6.05337029e-15, 6.61372576e-15\n",
    ", 7.14210116e-15, 6.94968538e-15, 1.85742697e-14, 3.52090532e-14\n",
    ", 1.48785528e-14, 6.23577963e-15, 5.66229647e-15, 5.39212323e-15\n",
    ", 5.32890121e-15, 5.54967559e-15, 5.29485491e-15, 5.65665900e-15\n",
    ", 5.31337965e-15, 5.30139224e-15, 5.21434237e-15, 5.61739646e-15\n",
    ", 5.62673191e-15, 5.68441483e-15, 5.43332729e-15, 5.34563989e-15\n",
    ", 5.69510011e-15, 6.43038706e-15, 5.52069097e-15, 5.22891308e-15\n",
    ", 5.06513758e-15, 5.15715319e-15, 5.32484298e-15, 5.37071225e-15\n",
    ", 5.24099974e-15, 5.14413780e-15, 5.05322799e-15, 5.27277366e-15\n",
    ", 5.17209094e-15, 5.19895605e-15, 5.04662049e-15, 5.13492402e-15\n",
    ", 5.39573281e-15, 5.13639899e-15, 5.29696474e-15, 5.29749076e-15\n",
    ", 5.23187737e-15, 5.14179424e-15, 1.44011463e-14, 2.29753019e-14\n",
    ", 8.85041560e-15, 5.16398069e-15, 5.09075672e-15, 5.06681957e-15\n",
    ", 5.17284653e-15, 4.99688083e-15, 5.01988585e-15, 5.07952302e-15\n",
    ", 4.97188381e-15, 5.17733558e-15, 4.97124292e-15, 4.96910679e-15\n",
    ", 4.96283207e-15, 5.07193856e-15, 4.80712108e-15, 4.97630935e-15\n",
    ", 4.93727883e-15, 4.84091247e-15, 5.07370238e-15, 4.76459850e-15\n",
    ", 4.86678392e-15, 5.03907955e-15, 4.91645908e-15, 4.99691785e-15\n",
    ", 4.81326372e-15, 5.56398292e-15, 5.40286001e-15, 4.91762834e-15\n",
    ", 4.96042200e-15, 4.86125369e-15, 5.04306529e-15, 4.89229744e-15\n",
    ", 4.93924928e-15, 4.91889752e-15, 4.92336366e-15, 4.91476256e-15\n",
    ", 4.90731383e-15, 4.79751988e-15, 4.96181659e-15, 5.04353794e-15]\n",
    "\n",
    "#plot the data with ploty:\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "freqs = [i/2 for i in range(0, 280)]\n",
    "prominence_lvl_pos = 50\n",
    "prominence_pos=(max(one_psd) - min(one_psd)) / prominence_lvl_pos\n",
    "noisy_freqs_indexes, _ = find_peaks(one_psd, prominence=prominence_pos)\n",
    "noisy_freqs_indexes = [int(i) for i in noisy_freqs_indexes]\n",
    "\n",
    "for i in range(0,2):\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=freqs, y=one_psd, name='psd'))\n",
    "    fig.update_layout(title=' PSD', xaxis_title='Frequency', yaxis_title='Amplitude',\n",
    "            yaxis = dict(\n",
    "            showexponent = 'all',\n",
    "            exponentformat = 'e'))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=[freqs[noisy_freqs_indexes[0]]], y=[one_psd[noisy_freqs_indexes[0]]], mode='markers', name='peaks'))\n",
    "\n",
    "    if i == 0:\n",
    "        fig.update_yaxes(type=\"log\")\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_head_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool('False')\n",
    "\n",
    "#convert \"false\" to boolean:\n",
    "import ast\n",
    "t = ast.literal_eval('False')\n",
    "t\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "megqc_empty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
